---
title: 机器学习笔记-----线性回归
categories: 
  - 机器学习
tags:
  - 机器学习
  - python3
  
date: 2019-01-14
mathjax: true

---


## 线性模型的基本形式

给定由d 个属性描述的示例$\vec x = (x_1,x_2,...x_d)$ ， 其中$x_i$是x 在第i个属性上的取值，线性模型(linear model)试图学得一个通过属性的线性组合来进行预测的函数，即:

$$f(\vec x) = \omega_1 x_1 + \omega_2 x_2 +\cdots + \omega_d x_d+d + b  \tag{1}$$

写成向量式：

$$f(\vec x ) = \vec\omega^T \vec x  +b \tag{2} $$

## 线性回归原理

给定数据集$D = \{(\vec x_1 ,y_1),(\vec x_2,y_2),\cdots,(\vec x_m,y_m)\}$,线性回归就是使用$\vec x$得到一个线性模型是的输出$y'$ 无限接近与$y$，即：

$$f(\vec x ) = \vec\omega^T \vec x + b \simeq y \tag{3}$$

这里忽略了下标，即每一个输入$\vec x$ 通过线性回归模型的式子$(3)$得到的$y'$ 都无限接近与数据集中的$y$

## 代价函数

那么如何确定公式$(3)$中的$\vec\omega$和$b$呢?

如果我们的$f(x)$ 与$y$的差别足够小，那么我们就可以带入到公式$(3)$中直接解出$\vec\omega$和$b$来:

$$min\sum|f(x)-y|$$

而在实际运用过程中我们一般用平方误差来做性能度量：

$$min \sum(f(x)-y)^2 \tag{4}$$

当然也有其他的性能度量方式，只是平方误差这个在线性回归中最常用，这个求解线性回归参数的方法也叫做**最小二乘法**

令：

$$E(w,b) = \sum(f(x)-y)^2 \tag{5}$$

$E(w,b)$在这里就叫做**代价函数**,吴恩达老师的机器学习课程中稍做了调整：

$$E(w,b) = \frac{1}{2m} \sum_{i=1}^m (f(x)-y)^2 \tag{5}$$

我们只需要对$E(w,b)$求偏导就能够求得最小值，这里假设$E(w,b)$是**凸函数**：



$$ \frac{\partial E(\omega,b) }{\partial \omega} = 2(\omega \sum_{1=1}^m x_i^2 - \sum_{i=1}^m (y_i-b)x_i) $$

$$  \frac{\partial E(\omega,b) }{\partial b} = 2(mb - \sum_{i=1}^m (y_i-\omega x_i)) $$ 



## 梯度下降法

在讲梯度下降法之前，我们先讲如何直接解出$E(\omega,b)$的最小值：

![解方程](/myphoto/ml1.png)

![解方程2](/myphoto/ml2.png)

![解方程3](/myphoto/ml3.png)

显然这种方法比较复杂，貌似计算量也很大?

下面讲另一种求解$E$最小值的方式，**梯度下降法**

该方法的核心思想是同步更新$\vec\omega,b$中每个值：

$$\omega_i = \omega_i -\alpha \frac{\partial E }{\partial \omega_i} ,(i=1,2,...m)$$

$\alpha​$称作**学习率**,它的值太小会增加求解的时间，太大会容易造成过拟合。

该方法像是一种穷举，一步一步的找到最小值。



关于这种方法的实现我们将会在逻辑回归中演示，下面讲一下Scikit-Learn关于线性回归的用法。



## LinearRegression



参考sklearn的官方文档中文版：http://cwiki.apachecn.org/pages/viewpage.action?pageId=12681716

`linear_model.LinearRegression`类的成员函数 `fit` 以数组X和y为输入，并将线性模型的系数$\omega$存储在其成员变量`coef_` 中：

```python
>>> from sklearn import linear_model
>>> reg = linear_model.LinearRegression()
>>> reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
>>> reg.coef_
array([ 0.5,  0.5])
```

> 然而，最小二乘的系数估计依赖于模型特征项的独立性。当特征项相关并且设计矩阵X 的列近似的线性相关时，设计矩阵便接近于一个奇异矩阵(不是满秩，列数过多，会有多个最优解，需要**正则化**)，因此最小二乘估计对观测点中的随机误差变得高度敏感，产生大的方差。

## 造轮子

```python

import numpy as np
import matplotlib.pyplot as plt

class SimpleLR:

    def __init__(self):
        self._a = None
        self._b = None

    def fit(self,x_train,y_train):
        if x_train.ndim == 1 and len(x_train) == len(y_train):
            self._a = (np.mean(x_train) * np.mean(y_train) - np.mean(x_train * y_train)) / \
                      ((np.mean(x_train)**2) - np.mean(x_train**2))
            self._b = np.mean(y_train) - self._a * np.mean(x_train)
            print(self._a,self._b)

    def predict(self,x_test):
        if x_test.ndim == 1 and self._a is not None and self._b is not None:
            return self._a * x_test + self._b

def main():
    rng = np.random.RandomState(1)
    x_train = 10 * rng.rand(50) # 生成均匀分布的50个数
    y_train = 2 * x_train - 5 + rng.randn(50) # randn是标准正太分布
    x_test = np.linspace(0, 10, 1000)
    slr = SimpleLR()
    slr.fit(x_train, y_train)
    y = slr.predict(x_test)
    # 画图
    fig = plt.figure()
    plt.scatter(x_train, y_train)
    plt.plot(x_test,y)
    plt.show()
    fig.savefig('fig.png')


if __name__ == '__main__':
    main()
```
输出：
```
2.027208810360695 -4.998577085553202
```
可见训练效果还是不错的

![可视化](/myphoto/liner_reg.png)