<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>机器学习笔记-----数据预处理</title>
      <link href="/2019/05/17/ji-qi-xue-xi-bi-ji-shu-ju-yu-chu-li/"/>
      <url>/2019/05/17/ji-qi-xue-xi-bi-ji-shu-ju-yu-chu-li/</url>
      
        <content type="html"><![CDATA[<p>数据预处理是机器学习中非常重要的步骤，为了能正确的分析数据，得到最佳的机器学习算法，拿到数据后我们一般来说都需要对数据进行预处理。数据预处理包括以下几个步骤：</p><ul><li>导入需要的库</li><li>导入数据集</li><li>处理丢失数据</li><li>解析分类数据</li><li>拆分数据集分成训练集合和测试集合</li><li>特征缩放</li></ul><a id="more"></a><p><img src="/myphoto/ML-1.png" alt="ML-1"></p><p><em>图片来源与github项目<a href="https://github.com/MLEveryday/100-Days-Of-ML-Code" target="_blank" rel="noopener">机器学习100天中文翻译</a>, 喜欢的可以去点个star哦</em></p><h2 id="1-导入库"><a href="#1-导入库" class="headerlink" title="1. 导入库"></a>1. 导入库</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br></pre></td></tr></table></figure><h2 id="2-导入数据集"><a href="#2-导入数据集" class="headerlink" title="2. 导入数据集"></a>2. 导入数据集</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dataset = pd.read_csv(<span class="hljs-string">r'E:\Downloads\100-Days-Of-ML-Code-master\100-Days-Of-ML-Code-master\datasets\Data.csv'</span>)</span><br><span class="line">X = dataset.iloc[ : , :<span class="hljs-number">-1</span>].values <span class="hljs-comment"># loc 表示标签索引行数据</span></span><br><span class="line">Y = dataset.iloc[ : , <span class="hljs-number">-1</span>].values <span class="hljs-comment"># iloc 表示按照行号号索引</span></span><br><span class="line">print(Y)</span><br></pre></td></tr></table></figure><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;No&apos; &apos;Yes&apos; &apos;No&apos; &apos;No&apos; &apos;Yes&apos; &apos;Yes&apos; &apos;No&apos; &apos;Yes&apos; &apos;No&apos; &apos;Yes&apos;]</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset</span><br></pre></td></tr></table></figure><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CountryAgeSalaryPurchased</span><br><span class="line">France4472000No</span><br><span class="line">Spain2748000Yes</span><br><span class="line">Germany3054000No</span><br><span class="line">Spain3861000No</span><br><span class="line">Germany40Yes</span><br><span class="line">France3558000Yes</span><br><span class="line">Spain52000No</span><br><span class="line">France4879000Yes</span><br><span class="line">Germany5083000No</span><br><span class="line">France3767000Yes</span><br></pre></td></tr></table></figure><h2 id="3-处理丢失数据"><a href="#3-处理丢失数据" class="headerlink" title="3. 处理丢失数据"></a>3. 处理丢失数据</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> Imputer</span><br><span class="line"><span class="hljs-comment"># axis=0表示按列的平均值进行填充</span></span><br><span class="line">imputer = Imputer(missing_values = <span class="hljs-string">"NaN"</span>, strategy = <span class="hljs-string">"mean"</span>, axis = <span class="hljs-number">0</span>)</span><br><span class="line">imputer = imputer.fit(X[ : , <span class="hljs-number">1</span>:<span class="hljs-number">3</span>]) <span class="hljs-comment"># 处理缺失值</span></span><br><span class="line">X[ : , <span class="hljs-number">1</span>:<span class="hljs-number">3</span>] = imputer.transform(X[ : , <span class="hljs-number">1</span>:<span class="hljs-number">3</span>]) <span class="hljs-comment"># 转换</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure><p>上述代码使用的是样本的平均值来代替缺失值, 也可以使用中位数、众数，接口详细信息参见<code>help(Imputer)</code></p><h2 id="4-解析分类数据"><a href="#4-解析分类数据" class="headerlink" title="4. 解析分类数据"></a>4. 解析分类数据</h2><p><strong>为什么要独热编码？</strong></p><p>使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，会让特征之间的距离计算更加合理。  </p><p><strong>为什么特征向量要映射到欧式空间？</strong></p><p>将离散特征通过one-hot编码映射到欧式空间，是因为，在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间</p><p>关于<a href="https://www.cnblogs.com/king-lps/p/7846414.html" target="_blank" rel="noopener">OneHotEncoder独热编码和 LabelEncoder标签编码</a></p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> LabelEncoder, OneHotEncoder</span><br><span class="line">labelencoder_X = LabelEncoder() <span class="hljs-comment"># 对与不连续的变量进行编号</span></span><br><span class="line">X[ : , <span class="hljs-number">0</span>] = labelencoder_X.fit_transform(X[ : , <span class="hljs-number">0</span>])</span><br><span class="line"><span class="hljs-comment">#Creating a dummy variable</span></span><br><span class="line">onehotencoder = OneHotEncoder(categorical_features = [<span class="hljs-number">0</span>]) <span class="hljs-comment"># 进行虚位掩码</span></span><br><span class="line">X = onehotencoder.fit_transform(X).toarray()</span><br><span class="line">labelencoder_Y = LabelEncoder()</span><br><span class="line">Y =  labelencoder_Y.fit_transform(Y)</span><br><span class="line">print(Y)</span><br><span class="line">print(X[:,<span class="hljs-number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[0 1 0 0 1 1 0 1 0 1]</span><br><span class="line">[0. 1. 1. 1. 1. 0. 1. 0. 1. 0.]</span><br></pre></td></tr></table></figure><h2 id="5-拆分数据集分成训练集合和测试集合"><a href="#5-拆分数据集分成训练集合和测试集合" class="headerlink" title="5. 拆分数据集分成训练集合和测试集合"></a>5. 拆分数据集分成训练集合和测试集合</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 训练集和测试集</span></span><br><span class="line"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = <span class="hljs-number">0.2</span>, random_state = <span class="hljs-number">0</span>)</span><br></pre></td></tr></table></figure><p><strong>关于怎么拆分数据集</strong></p><p>拆分数据的目的是对模型进行评估，如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分成三部分，分别为<strong>训练集（training set）</strong>、<strong>验证集</strong>（validation set）和<strong>测试集（test set）</strong>, 比例一般是 6:2:2</p><p>但是，在许多实际应用中数据是不充足的。为了选择好的模型，可以采用交叉验证方法。</p><p><strong>交叉验证</strong>的基本想法是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。一般有 <strong>简单交叉验证， S折交叉验证， 留一交叉验证</strong>等方法</p><p><em>我们会在《模型评估与模型选择》一节在进行详细描述</em></p><h2 id="6-特征缩放"><a href="#6-特征缩放" class="headerlink" title="6. 特征缩放"></a>6. 特征缩放</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># 标准化</span></span><br><span class="line"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler</span><br><span class="line">sc_X = StandardScaler()</span><br><span class="line">X_train = sc_X.fit_transform(X_train) <span class="hljs-comment"># fit_transform 先训练数据得到方差期望等信息，再进行缩放</span></span><br><span class="line">X_test = sc_X.transform(X_test) <span class="hljs-comment"># transform直接进行数据缩放，用的是X_train的方差和期望</span></span><br></pre></td></tr></table></figure><p><strong>关于特征缩放</strong></p><p>特征缩放就是将所有的特征分布转成成同一数量级，示例数据中的年龄与薪水的方差、均值等分布特征相差很大，这样会导致年龄对模型的影响会很小，其实就是均一化。</p><p><code>sklearn</code>特征缩放<code>StandardScaler</code> 的计算公式:</p><p>$$  x_{stand} = \frac {x-mean(x)} {std} $$ </p><p>特征缩放的更多介绍:<br><a href="https://www.cnblogs.com/chaosimple/p/4153167.html" target="_blank" rel="noopener">关于使用sklearn进行数据预处理 —— 归一化/标准化/正则化</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="http://www.leafw.cn/2019/01/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0a-z%EF%BD%9E%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/" target="_blank" rel="noopener">机器学习A-Z～数据预处理</a></li><li><a href="https://github.com/MLEveryday/100-Days-Of-ML-Code" target="_blank" rel="noopener">机器学习100天</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 数学与统计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记-----线性回归</title>
      <link href="/2019/05/17/ji-qi-xue-xi-bi-ji-xian-xing-hui-gui/"/>
      <url>/2019/05/17/ji-qi-xue-xi-bi-ji-xian-xing-hui-gui/</url>
      
        <content type="html"><![CDATA[<h2 id="线性模型的基本形式"><a href="#线性模型的基本形式" class="headerlink" title="线性模型的基本形式"></a>线性模型的基本形式</h2><p>给定由d 个属性描述的示例$\vec x = (x_1,x_2,…x_d)$ ， 其中$x_i$是x在第i个属性上的取值，线性模型(linear model)试图学得一个通过属性的线性组合来进行预测的函数，即<br>公式（1）：<br>$$ f(\vec x) = \omega_1 x_1 + \omega_2 x_2 +\cdots + \omega_d x_d+d + b  $$ </p><p>写成向量式，公式（2）：<br>$$ f(\vec x) = \vec\omega^T \vec x  $$</p><a id="more"></a><p>这里的</p>$$\vec\omega^T = [\omega_1,\omega_2,\cdots,\omega_n]$$$$\vec x = \left[\begin{matrix}x_1 \\x_2 \\\cdots\\x_{n-1}\\1\end{matrix} \right]$$<p>式(2)是单个样本的模型，如果是多个样本，那么:</p>$$X = \left[\begin{matrix}x_1^{(1)} \ x_2^{(1)} \ x_1^{(1)} \ \cdots \ x_{n-1}^{(1)} \ 1 \\x_1^{(2)} \ x_2^{(2)} \ x_3^{(2)} \ \cdots \ x_{n-1}^{(2)} \ 1 \\x_1^{(3)} \ x_2^{(3)} \ x_3^{(3)} \ \cdots \ x_{n-1}^{(3)} \ 1 \\\cdots \\x_1^{(m)} \ x_2^{(m)} \ x_3^{(m)} \ \cdots \ x_{n-1}^{(m)} \ 1\end{matrix} \right] = \left[\begin{matrix}\vec x^{(1)T} \ 1 \\\vec x^{(2)T} \ 1 \\\vec x^{(3)T} \ 1 \\\cdots \\\vec x^{(m)T} \ 1 \\\end{matrix} \right]$$<p>因此多变量线性回归的公式就变成：</p><p>$$f(x)= X^T \vec\omega $$</p><h2 id="线性回归原理"><a href="#线性回归原理" class="headerlink" title="线性回归原理"></a>线性回归原理</h2><p>给定数据集$D = {(\vec x_1 ,y_1),(\vec x_2,y_2),\cdots,(\vec x_m,y_m)}$,线性回归就是使用$\vec x$得到一个线性模型是的输出$y’$ 无限接近与$y$，即：</p><p>公式（3）<br>$$f(\vec x ) = \vec\omega^T \vec x \simeq y $$<br>这里忽略了下标，即每一个输入$\vec x$ 通过线性回归模型的式子$(3)$得到的$y$ 都无限接近与数据集中的$y$</p><h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>那么如何确定公式$(3)$中的$\vec\omega$呢?</p><p>如果我们的$f(x)$ 与$y$的差别足够小，那么我们就可以带入到公式$(3)$中直接解出$\vec\omega$和$b$来:</p><p>$$min\sum|f(x)-y|$$</p><p>而在实际运用过程中我们一般用平方误差来做性能度量，公式（4）：</p><p>$$min \sum(f(x)-y)^2 $$</p><p>当然也有其他的性能度量方式，只是平方误差这个在线性回归中最常用，这个求解线性回归参数的方法也叫做<strong>最小二乘法</strong><br>令：</p><p>公式（5）<br>$$J (\omega) = \sum(f(x)-y)^2 $$</p><p>$J(\omega)$在这里就叫做<strong>代价函数</strong>,吴恩达老师的机器学习课程中稍做了调整：</p><p>$$J(\omega) = \frac{1}{2m} \sum_{i=1}^m (f(x)-y)^2 $$</p><p>我们只需要对$J(\omega)$求偏导就能够求得最小值，这里假设$J(\omega)$是<strong>凸函数</strong></p><h2 id="代价函数最小值"><a href="#代价函数最小值" class="headerlink" title="代价函数最小值"></a>代价函数最小值</h2><p>求最小值可以使用梯度下降法，在这之前，我们先讲如何直接解出$J(\omega)$的最小值：</p><h3 id="求导法"><a href="#求导法" class="headerlink" title="求导法"></a>求导法</h3><p><img src="/myphoto/ml1.png" alt=""></p><h3 id="矩阵法"><a href="#矩阵法" class="headerlink" title="矩阵法"></a>矩阵法</h3><p><img src="/myphoto/ml2.png" alt=""><br><img src="/myphoto/ml3.png" alt=""></p><p>显然这种方法比较复杂，而且逆矩阵也不一定存在，,貌似计算量也很大</p><p>下面讲另一种求解$J(\omega)$最小值的方式，</p><h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a><strong>梯度下降法</strong></h3><p>该方法的核心思想是同步更新$\vec\omega$中每个值：</p><p><code>for j in: len(w):</code><br>$$<br>\omega_j = \omega_j -\alpha \sum_{i=1}^m \frac{\partial J(\omega) }{\partial \omega_j}<br>$$</p><p>$\alpha$ 称作<strong>学习率</strong>,它的值太小会增加求解的时间，太大会容易造成过拟合。<br>该方法像是一种穷举，一步一步的找到最小值。<br>关于这种方法的实现我们将会在逻辑回归中演示，下面讲一下Scikit-Learn关于线性回归的用法。</p><h2 id="LinearRegression"><a href="#LinearRegression" class="headerlink" title="LinearRegression"></a>LinearRegression</h2><p><a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=12681716" target="_blank" rel="noopener">参考sklearn的官方文档中文版</a></p><p><code>linear_model.LinearRegression</code>类的成员函数 <code>fit</code> 以数组X和y为输入，并将线性模型的系数$\omega$存储在其成员变量<code>coef_</code> 中：</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> linear_model</span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>reg = linear_model.LinearRegression()</span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>reg.fit ([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>]], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>])</span><br><span class="line">LinearRegression(copy_X=<span class="hljs-keyword">True</span>, fit_intercept=<span class="hljs-keyword">True</span>, n_jobs=<span class="hljs-number">1</span>, normalize=<span class="hljs-keyword">False</span>)</span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span>reg.coef_</span><br><span class="line">array([ <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>])</span><br></pre></td></tr></table></figure><blockquote><p>然而，最小二乘的系数估计依赖于模型特征项的独立性。当特征项相关并且设计矩阵X 的列近似的线性相关时，设计矩阵便接近于一个奇异矩阵(不是满秩，列数过多，会有多个最优解，需要<strong>正则化</strong>)，因此最小二乘估计对观测点中的随机误差变得高度敏感，产生大的方差。</p></blockquote><h2 id="手动实现线性回归"><a href="#手动实现线性回归" class="headerlink" title="手动实现线性回归"></a>手动实现线性回归</h2><p>这里我们使用矩阵方法求参数</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">boston = datasets.load_boston()</span><br><span class="line">X = boston.data</span><br><span class="line">Y = boston.target</span><br><span class="line"></span><br><span class="line">X = X[Y &lt; <span class="hljs-number">50.0</span>]</span><br><span class="line">Y = Y[Y &lt; <span class="hljs-number">50.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split </span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = <span class="hljs-number">0.25</span>, random_state = <span class="hljs-number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># print(X.shape)</span></span><br><span class="line"><span class="hljs-comment"># print(Y.shape)</span></span><br><span class="line"><span class="hljs-comment"># (490, 13)</span></span><br><span class="line"><span class="hljs-comment"># (490,)</span></span><br><span class="line"><span class="hljs-comment"># 加一列截距</span></span><br><span class="line">X_train_b = np.insert(X_train,<span class="hljs-number">0</span>,values=<span class="hljs-number">1</span>,axis=<span class="hljs-number">1</span>)</span><br><span class="line">X_test_b = np.insert(X_test,<span class="hljs-number">0</span>,values=<span class="hljs-number">1</span>,axis=<span class="hljs-number">1</span>)</span><br><span class="line"><span class="hljs-comment"># print(y_train.shape)</span></span><br><span class="line">y_train_b = y_train.reshape(y_train.shape[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>)</span><br><span class="line"></span><br><span class="line">theta = np.linalg.inv(X_train_b.T.dot(X_train_b)).dot(X_train_b.T).dot(y_train_b)</span><br><span class="line">print(theta)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 使用sklearn</span></span><br><span class="line"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> linear_model</span><br><span class="line">reg = linear_model.LinearRegression()</span><br><span class="line">reg.fit(X,Y)</span><br><span class="line">print(reg.intercept_)</span><br><span class="line">print(reg.coef_)</span><br><span class="line"><span class="hljs-comment"># print(dir(reg))</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数学与统计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记-----Logistic回归</title>
      <link href="/2019/01/23/ji-qi-xue-xi-bi-ji-logistic-hui-gui/"/>
      <url>/2019/01/23/ji-qi-xue-xi-bi-ji-logistic-hui-gui/</url>
      
        <content type="html"><![CDATA[<p>Logistic回归,有翻译为“逻辑回归”，但是它跟“逻辑”这个单词一点关系都没有，也不是解决回归问题的，是用来解决二分类的算法，最准确的翻译应该是“对数几率回归”</p><p>至于为什么叫对数几率刚回归，请看下面的推倒过程</p><a id="more"></a> <h2 id="线性回归回顾"><a href="#线性回归回顾" class="headerlink" title="线性回归回顾"></a>线性回归回顾</h2><p>先来看一个简单的线性回归模型：</p><p>$$ f(x) = \vec\omega^T \vec{x}+b  $$<br>这里的</p><p>$$ \vec\omega^T = [\omega_1,\omega_2,\cdots,\omega_n] $$</p>$$ \vec x  = \left[\begin{matrix}x_1 \\x_2  \\\cdots\\x_n  \end{matrix} \right] $$<p>一般来说给定的数据集是：$D={(\vec x^{(1)},y^{(1)}),(\vec x^{(2)},y^{(2)}),⋯,(\vec x^{(m)},y^{(m)})}$, 即有m组数据，n个未知数，因此多变量线性回归的公式是：</p><p>$$f(x)= X^T \vec\omega $$</p><p>其中：</p>$$X = \left[\begin{matrix}x_1^{(1)} \   x_2^{(1)} \ x_1^{(1)} \ \cdots \ x_n^{(1)} \ 1 \\x_1^{(2)} \   x_2^{(2)} \ x_3^{(2)} \ \cdots \ x_n^{(2)} \ 1 \\x_1^{(3)} \   x_2^{(3)} \ x_3^{(3)} \ \cdots \ x_n^{(3)} \ 1 \\\cdots \\x_1^{(m)} \   x_2^{(m)} \ x_3^{(m)} \ \cdots \ x_n^{(m)} \ 1 \end{matrix} \right] =  \left[\begin{matrix}\vec x^{(1)T} \ 1 \\\vec x^{(2)T} \ 1 \\\vec x^{(3)T} \ 1 \\\cdots \\\vec x^{(m)T} \ 1 \\\end{matrix} \right]$$<p>展开式：</p><p>$$\omega_1 x_1^{(1)} + \omega_2 x_2^{(1)} + \cdots + \omega_n x_n^{(1)} +b = y^{(1)} $$</p><p>$$\omega_1 x_1^{(2)} + \omega_2 x_2^{(2)} + \cdots + \omega_n x_n^{(2)} + b = y^{(2)}$$</p><p>$$\cdots$$</p><p>$$\omega_1 x_1^{(m)} + \omega_2 x_2^{(m)} + \cdots + \omega_n x_n^{(m)} +b = y^{(m)}$$</p><p>我们为了便于推导，忽略了下标，只用了一组$(\vec{x},y)$,然后把$b$当做$\omega_0$，而此时$x_0 = 1$：</p><p>$$f(x)=\vec\omega^T \vec{x} $$</p><h2 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h2><p>线性回归很显然是一个连续的值，既然要解决二分类的问题，那最好是换成一个$0/1$的值</p><p>这里引出一个sigmoid函数：<br>$$g(z) = \frac {1}{1+e^{-z}}$$</p><p><img src="/myphoto/logtic.png" alt="logtic"></p><p>令：</p><p>$$z =\vec\omega^T \vec{x}+b $$</p><p>$$f(x) = g(z) = \frac {1}{1+e^{-\vec\omega^T \vec{x}}} $$</p><p>我们就引出了我们的模型：</p><p>$$ln \frac{y}{1-y} =  \vec\omega^T \vec{x}+b $$</p><p>上式中$\frac{y}{1-y}$ 称为几率，反应x获得正例(y)的相对可能性，由此可以看出，模型实际上是在用线性回归模型的预测结果去逼近真实值$y$的对数几率，因此该模型就称为<strong>对数几率回归模型</strong></p><h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>由于$y$的值为0,1，我们将$y$视为后验概率：</p><p>$$p(y=1 |  x;\omega) = f(x) $$</p><p>$$p(y=0 |  x;\omega) = 1- f(x) $$</p><p>$f(x)$越大，代表着预测值与$y$越接近，误差就越小，带入到公式中：</p><p>$$p(y=1 |  x;\omega) = \frac {e^{\vec\omega^T \vec{x}}}{1+e^{\vec\omega^T \vec{x}}} $$</p><p>$$p(y=0 |  x;\omega) = \frac {1}{1+e^{\vec\omega^T \vec{x}}}  $$</p><p>为了确定$\omega$用对数似然函数：</p><p>$$ \ell(\omega) = lnp(y|x ;\omega)$$</p><p>可以合并到一起：</p><p>$$ p(y|x ;\omega) = f(x)^y(1-f(x))^{(1-y)}$$</p><p>带入到对数似然函数中：</p><p>$$ln p(y|x ;\omega) = y lnf(x) + (1-y)ln(1-f(x)) $$</p><p>我们得到的代价函数：</p><p>$$J(\omega) = - \frac{1}{m} \sum_{i=1}^m [ y^{(i)} lnf(x^{(i)}) + (1-y^{(i)})ln(1-f(x^{(i)})) ] $$</p><p>$J(\omega)$越小，就说明$f(x) = 1$的概率越大，因此只要求出$J(\omega)$的<strong>最小值</strong></p><blockquote><p>ps：</p><p>1）其实正常推倒出来的J(w)是没有前面的负号的，这里为了习惯，用梯度下降法求最小值，所以加了一个负号</p><p>2） 之前推倒时候忽略了上标，但是我们在最后的损失函数中为了完整性就把上标加了上去</p></blockquote><h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p>对代价函数求导的过程就不写了，直接写最后的结果:</p><p>$$\frac{\partial J(\omega)}{\partial\omega_j} =\frac{1}{m} \sum_{i=1}^m (f(x^{(i)}) - y^{(i)})x_j^{(i)}  $$</p><p>$$\omega_{j+1} := \omega_j- \frac{1}{m}\alpha\sum_{i=1}^m(f(x^{(i)}) - y^{(i)})x_j^{(i)} $$</p><h2 id="sklearn代码实现逻辑回归"><a href="#sklearn代码实现逻辑回归" class="headerlink" title="sklearn代码实现逻辑回归"></a>sklearn代码实现逻辑回归</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset = pd.read_csv(<span class="hljs-string">r'E:\Downloads\100-Days-Of-ML-Code-master\Social_Network_Ads.csv'</span>)</span><br><span class="line">dataset.head(<span class="hljs-number">5</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = dataset.iloc[:,[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]].values</span><br><span class="line">Y = dataset.iloc[:,<span class="hljs-number">4</span>].values</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = <span class="hljs-number">0.25</span>, random_state = <span class="hljs-number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler</span><br><span class="line">sc = StandardScaler()</span><br><span class="line">X_train = sc.fit_transform(X_train)</span><br><span class="line">X_test = sc.transform(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression</span><br><span class="line">classifier = LogisticRegression()</span><br><span class="line">classifier.fit(X_train, y_train)</span><br><span class="line"><span class="hljs-comment"># classifier.coef_</span></span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred = classifier.predict(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix</span><br><span class="line">cm = confusion_matrix(y_test, y_pred)</span><br></pre></td></tr></table></figure><p>可视化</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> matplotlib.colors <span class="hljs-keyword">import</span> ListedColormap</span><br><span class="line">X_set,y_set=X_train,y_train</span><br><span class="line">X1,X2=np. meshgrid(np. arange(start=X_set[:,<span class="hljs-number">0</span>].min()<span class="hljs-number">-1</span>, stop=X_set[:, <span class="hljs-number">0</span>].max()+<span class="hljs-number">1</span>, step=<span class="hljs-number">0.01</span>),</span><br><span class="line">                   np. arange(start=X_set[:,<span class="hljs-number">1</span>].min()<span class="hljs-number">-1</span>, stop=X_set[:,<span class="hljs-number">1</span>].max()+<span class="hljs-number">1</span>, step=<span class="hljs-number">0.01</span>))</span><br><span class="line">plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),</span><br><span class="line">             alpha = <span class="hljs-number">0.75</span>, cmap = ListedColormap((<span class="hljs-string">'red'</span>, <span class="hljs-string">'green'</span>)))</span><br><span class="line">plt.xlim(X1.min(),X1.max())</span><br><span class="line">plt.ylim(X2.min(),X2.max())</span><br><span class="line"><span class="hljs-keyword">for</span> i,j <span class="hljs-keyword">in</span> enumerate(np. unique(y_set)):</span><br><span class="line">    plt.scatter(X_set[y_set==j,<span class="hljs-number">0</span>],X_set[y_set==j,<span class="hljs-number">1</span>],</span><br><span class="line">                c = ListedColormap((<span class="hljs-string">'red'</span>, <span class="hljs-string">'green'</span>))(i), label=j)</span><br><span class="line"></span><br><span class="line">plt. title(<span class="hljs-string">' LOGISTIC(Training set)'</span>)</span><br><span class="line">plt. xlabel(<span class="hljs-string">' Age'</span>)</span><br><span class="line">plt. ylabel(<span class="hljs-string">' Estimated Salary'</span>)</span><br><span class="line">plt. legend()</span><br><span class="line">plt. show()</span><br><span class="line"></span><br><span class="line">X_set,y_set=X_test,y_test</span><br><span class="line">X1,X2=np. meshgrid(np. arange(start=X_set[:,<span class="hljs-number">0</span>].min()<span class="hljs-number">-1</span>, stop=X_set[:, <span class="hljs-number">0</span>].max()+<span class="hljs-number">1</span>, step=<span class="hljs-number">0.01</span>),</span><br><span class="line">                   np. arange(start=X_set[:,<span class="hljs-number">1</span>].min()<span class="hljs-number">-1</span>, stop=X_set[:,<span class="hljs-number">1</span>].max()+<span class="hljs-number">1</span>, step=<span class="hljs-number">0.01</span>))</span><br><span class="line"></span><br><span class="line">plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),</span><br><span class="line">             alpha = <span class="hljs-number">0.75</span>, cmap = ListedColormap((<span class="hljs-string">'red'</span>, <span class="hljs-string">'green'</span>)))</span><br><span class="line">plt.xlim(X1.min(),X1.max())</span><br><span class="line">plt.ylim(X2.min(),X2.max())</span><br><span class="line"><span class="hljs-keyword">for</span> i,j <span class="hljs-keyword">in</span> enumerate(np. unique(y_set)):</span><br><span class="line">    plt.scatter(X_set[y_set==j,<span class="hljs-number">0</span>],X_set[y_set==j,<span class="hljs-number">1</span>],</span><br><span class="line">                c = ListedColormap((<span class="hljs-string">'red'</span>, <span class="hljs-string">'green'</span>))(i), label=j)</span><br><span class="line"></span><br><span class="line">plt. title(<span class="hljs-string">' LOGISTIC(Test set)'</span>)</span><br><span class="line">plt. xlabel(<span class="hljs-string">' Age'</span>)</span><br><span class="line">plt. ylabel(<span class="hljs-string">' Estimated Salary'</span>)</span><br><span class="line">plt. legend()</span><br><span class="line">plt. show()</span><br></pre></td></tr></table></figure><p><img src="/myphoto/output_12_0.png" alt="png"></p><p><img src="/myphoto/output_12_1.png" alt="png"></p>]]></content>
      
      
      <categories>
          
          <category> 数学与统计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>沈向洋：You Are What You Write</title>
      <link href="/2018/03/03/you-are-what-you-write/"/>
      <url>/2018/03/03/you-are-what-you-write/</url>
      
        <content type="html"><![CDATA[<h2 id="You-Are-What-You-Write"><a href="#You-Are-What-You-Write" class="headerlink" title="You Are What You Write"></a>You Are What You Write</h2><p><em>原文：<a href="https://www.linkedin.com/pulse/you-what-write-harry-shum?from=timeline&amp;isappinstalled=0" target="_blank" rel="noopener">https://www.linkedin.com/pulse/you-what-write-harry-shum?from=timeline&amp;isappinstalled=0</a></em></p><p>Are Twitter, PowerPoint, Facebook, Instagram and texting eroding our ability to think?</p><p>There is a Chinese proverb that says “见文如见人,” which literally means “reading the document is the same as seeing the author.” If we are what we write, then who have we, as a society, become?</p><p>I was sitting in a technical review recently, listening to one of our reviewers grill the engineer who was presenting: Why did you choose that design? Why is the service showing bad results? How many users will switch to the solution?</p><a id="more"></a> <p>The presenter’s answers lacked depth. It seemed like he hadn’t done enough rigorous thinking, the kind where you sit quietly, sift through research, contemplate options, determine what you know, don’t know and where more work is required. The kind of thinking I did as a young researcher when peers took me and my work apart when I took short cuts. Back then, I practiced a disciplined approach, spending hours just thinking, and even more hours on the hardest part—writing it down.</p><p>Today, long-form writing is being replaced. Tweets pass for dialogue. PowerPoint condenses thoughts to bullets. Words have been traded for emojis and GIFs. And we’ve become addicted to the noise. What happens in an Internet minute? 16 million text messages. 1.8 million snaps. 452,000 Tweets. 156 million emails. Who has time to think, let alone write?</p><p>And maybe we, in the technology industry, have shaped this reality. We created the phones, apps and 24/7-connected world. We’ve enabled society to put down the pen. The only writing I do today is email or quick WeChat posts.</p><p>So now I worry that we’re losing a valuable tool that helps us to think deeply, express who we are at our greatest and expand the intellect of those around us. And for us in the technical community, this is especially troubling. The stakes are higher than ever before with AI. We’re under enormous pressure to ship quickly, to achieve more, faster, but we can’t do this at the expense of the highest engineering quality. We have to think carefully about consequences and alternatives. Who gets blamed when a self-driving car hits someone? The engineer who wrote the code is the driver. Who is accountable for the AI algorithm with bias? The engineer who created the AI.</p><p>I see fewer engineers writing and sharing deep thinking, but this is what will lead to far more true innovation across the industry. How will we achieve the big transformative breakthroughs versus the incremental milestones?</p><p>By writing. Because the way to think is actually to write.</p><p>Putting pen to paper forces you to develop and refine your thinking by iterating, revising and exploring alternatives. Anyone who can think deeply can write beautiful code, inspiring papers or develop the plan to bring the next big thing to life. I encourage you to read Reid Hoffman’s Series B pitch for LinkedIn in which he shares the thinking that helped him succeed. At the time, he shares that a partner in a venture firm was exposed to around 5,000 pitches, looked more closely at 600 to 800, and did between 0 and 2 deals.</p><p>Writing offers the possibility to create lasting artifacts. I think of papers I published that endure, albeit perhaps as reference materials. Plenoptic Sampling. Lazy Snapping. Poisson Matting. These are my work’s contribution to the field of computer vision and graphics. They will survive me and, if I’m lucky, even help shape a mind or two.</p><p>One of my favorite professors at Carnegie Mellon, Takeo Kanade, said that you have to write research papers like detective novels. You need story, suspense, surprise and ‘aha’ to explain your ideas to peers, to inspire others to contribute and advance your work and the whole field.</p><p>Writing is an equalizer to get the best from the whole team. At Amazon, presentations are done with the six-page paper. Meetings kick off with everyone reading followed by comments and questions to the author. Everyone operates from the same context, and introverts, extroverts and non-native speakers have an equal chance to get their thinking across. It’s not about the presenter’s personality, but the words.</p><p>Ultimately, writing helps make you successful. You might be the smartest person with the best idea, but if you can’t communicate your thinking in a compelling way, you won’t get far. Two engineers in our AI+R team who inspire me with their regular writing habits are Bill Ramsey and Ronny Kohavi. Bill has written over 250 blog posts at Microsoft, benefitting our entire technical community. With Ronny, you don’t even need to meet him—his highly cited A/B test experimentation papers say it all, and he’s publishing for the benefit of the industry on LinkedIn.</p><p>As you’re reading this, you may be logging your objections: I need to drive results, so I need to go straight to code. I’m known for my code, so I don’t need to write papers. I’m not a native speaker, and I speak better with my code. I don’t know what to write about. I don’t have time… But please set them aside—for your own success, for your company’s, for the industry’s advancement—and start writing.</p><p>I see so many occasions for building long-form writing back into the engineering culture—planning documents, project proposals, technology LRP’s, review articles—to inspire us to work together, collectively creating and cultivating big ideas and big thinking.</p><p>I took a first step recently, writing a research paper with my colleagues Xiaodong He and Di Li, From Eliza to XiaoIce: Challenges and Opportunities with Social Chatbots, for the first time in years, so please no judgment, only constructive feedback!</p><p>I challenge everyone reading this piece to write 500 words per week. If you’ve got an idea or you see a problem, write your proposal and share it!</p><p>Let’s rewrite our standards for thought leadership and engineering quality by writing more!</p>]]></content>
      
      
      <categories>
          
          <category> 阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 摘抄 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWK简明教程</title>
      <link href="/2018/02/16/awk-jian-ming-jiao-cheng/"/>
      <url>/2018/02/16/awk-jian-ming-jiao-cheng/</url>
      
        <content type="html"><![CDATA[<p>AWK是一种处理文本文件的语言，是一个强大的文本分析工具。之所以叫AWK是因为其取了三位创始人 Alfred Aho，Peter Weinberger, 和 Brian Kernighan 的 Family Name 的首字符。</p><a id="more"></a><h3 id="awk命令格式和选项"><a href="#awk命令格式和选项" class="headerlink" title="awk命令格式和选项"></a>awk命令格式和选项</h3><p>语法形式</p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">awk [options] &apos;script&apos; var=value file(s)</span><br><span class="line">awk [options] -f scriptfile var=value file(s)</span><br></pre></td></tr></table></figure><p>常用命令选项</p><ul><li>-F fs   fs指定输入分隔符，fs可以是字符串或正则表达式，如-F:</li><li>-v var=value   赋值一个用户定义变量，将外部变量传递给awk</li><li>-f scripfile  从脚本文件中读取awk命令</li><li>-m[fr] val   对val值设置内在限制，-mf选项限制分配给val的最大块数目；-mr选项限制记录的最大数目。这两个功能是Bell实验室版awk的扩展功能，在标准awk中不适用。</li></ul><p><strong>工作原理</strong></p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hawk &apos;BEGIN&#123; commands &#125; pattern&#123; commands &#125; END&#123; commands &#125;&apos;</span><br></pre></td></tr></table></figure><h3 id="内建变量"><a href="#内建变量" class="headerlink" title="内建变量"></a>内建变量</h3><table><thead><tr><th>变量</th><th>详细说明</th></tr></thead><tbody><tr><td>$0</td><td>当前记录（这个变量中存放着整个行的内容）</td></tr><tr><td>$1~$n</td><td>当前记录的第n个字段，字段间由FS分隔</td></tr><tr><td>FS</td><td>输入字段分隔符 默认是空格或Tab (<strong>也可以是-F</strong>)</td></tr><tr><td>NF</td><td>当前记录中的字段个数，就是有多少列</td></tr><tr><td>NR</td><td>已经读出的记录数，就是行号，从1开始，如果有多个文件话，这个值也是不断累加中。</td></tr><tr><td>FNR</td><td>当前记录数，与NR不同的是，这个值会是各个文件自己的行号</td></tr><tr><td>RS</td><td>输入的记录分隔符， 默认为换行符</td></tr><tr><td>OFS</td><td>输出字段分隔符， 默认也是空格</td></tr><tr><td>ORS</td><td>输出的记录分隔符，默认为换行符</td></tr><tr><td>FILENAME</td><td>当前输入文件的名字</td></tr></tbody></table><h3 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a>模式匹配</h3><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-meta">$</span> awk '$6 ~ /FIN|TIME/ || NR==1 &#123;print NR,$4,$5,$6&#125;' OFS="\t" netstat.txt</span><br></pre></td></tr></table></figure><h3 id="统计"><a href="#统计" class="headerlink" title="统计"></a>统计</h3><p>下面的命令计算所有的C文件，CPP文件和H文件的文件大小总和。</p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls -l  *.cpp *.c *.h | awk &apos;&#123;sum+=$5&#125; END &#123;print sum&#125;&apos;</span><br><span class="line">2511401</span><br></pre></td></tr></table></figure><p>统计每个用户的进程的占了多少内存</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-meta">$</span> ps aux | awk 'NR!=1&#123;a[$1]+=$6;&#125; END &#123; for(i in a) print i ", " a[i]"KB";&#125;'</span><br><span class="line">dbus, 540KB</span><br><span class="line">mysql, 99928KB</span><br><span class="line">www, 3264924KB</span><br><span class="line">root, 63644KB</span><br><span class="line">hchen, 6020KB</span><br></pre></td></tr></table></figure><p><strong>控制结构和脚本语言待补充</strong></p><p><strong>参考链接：</strong></p><p><a href="http://man.linuxde.net/awk" target="_blank" rel="noopener">http://man.linuxde.net/awk</a><br><a href="https://coolshell.cn/articles/9070.html" target="_blank" rel="noopener">https://coolshell.cn/articles/9070.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 计算机 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> awk </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
