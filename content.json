{"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"archives","text":"","link":"/archives/index.html"},{"title":"关于我","text":"github: https://github.com/JeWeizhong email: zhmqsnq90@foxmail.com QQ: 584728914 wechat: 商业转载请联系作者获得授权, 非商业转载请注明出处","link":"/about/index.html"},{"title":"friends","text":"","link":"/friends/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"AWK简明教程","text":"AWK是一种处理文本文件的语言，是一个强大的文本分析工具。之所以叫AWK是因为其取了三位创始人 Alfred Aho，Peter Weinberger, 和 Brian Kernighan 的 Family Name 的首字符。 awk命令格式和选项语法形式 12awk [options] &apos;script&apos; var=value file(s)awk [options] -f scriptfile var=value file(s) 常用命令选项 -F fs fs指定输入分隔符，fs可以是字符串或正则表达式，如-F: -v var=value 赋值一个用户定义变量，将外部变量传递给awk -f scripfile 从脚本文件中读取awk命令 -m[fr] val 对val值设置内在限制，-mf选项限制分配给val的最大块数目；-mr选项限制记录的最大数目。这两个功能是Bell实验室版awk的扩展功能，在标准awk中不适用。 工作原理 1hawk &apos;BEGIN{ commands } pattern{ commands } END{ commands }&apos; 内建变量 变量 详细说明 $0 当前记录（这个变量中存放着整个行的内容） $1~$n 当前记录的第n个字段，字段间由FS分隔 FS 输入字段分隔符 默认是空格或Tab (也可以是-F) NF 当前记录中的字段个数，就是有多少列 NR 已经读出的记录数，就是行号，从1开始，如果有多个文件话，这个值也是不断累加中。 FNR 当前记录数，与NR不同的是，这个值会是各个文件自己的行号 RS 输入的记录分隔符， 默认为换行符 OFS 输出字段分隔符， 默认也是空格 ORS 输出的记录分隔符，默认为换行符 FILENAME 当前输入文件的名字 模式匹配1$ awk '$6 ~ /FIN|TIME/ || NR==1 {print NR,$4,$5,$6}' OFS=\"\\t\" netstat.txt 统计下面的命令计算所有的C文件，CPP文件和H文件的文件大小总和。 12$ ls -l *.cpp *.c *.h | awk &apos;{sum+=$5} END {print sum}&apos;2511401 统计每个用户的进程的占了多少内存 123456$ ps aux | awk 'NR!=1{a[$1]+=$6;} END { for(i in a) print i \", \" a[i]\"KB\";}'dbus, 540KBmysql, 99928KBwww, 3264924KBroot, 63644KBhchen, 6020KB 控制结构和脚本语言待补充 参考链接： http://man.linuxde.net/awkhttps://coolshell.cn/articles/9070.html","link":"/2018/05/16/awk-jian-ming-jiao-cheng/"},{"title":"我为什么要写博客","text":"You Are What You Write原文：https://www.linkedin.com/pulse/you-what-write-harry-shum?from=timeline&amp;isappinstalled=0 Are Twitter, PowerPoint, Facebook, Instagram and texting eroding our ability to think? There is a Chinese proverb that says “见文如见人,” which literally means “reading the document is the same as seeing the author.” If we are what we write, then who have we, as a society, become? I was sitting in a technical review recently, listening to one of our reviewers grill the engineer who was presenting: Why did you choose that design? Why is the service showing bad results? How many users will switch to the solution? The presenter’s answers lacked depth. It seemed like he hadn’t done enough rigorous thinking, the kind where you sit quietly, sift through research, contemplate options, determine what you know, don’t know and where more work is required. The kind of thinking I did as a young researcher when peers took me and my work apart when I took short cuts. Back then, I practiced a disciplined approach, spending hours just thinking, and even more hours on the hardest part—writing it down. Today, long-form writing is being replaced. Tweets pass for dialogue. PowerPoint condenses thoughts to bullets. Words have been traded for emojis and GIFs. And we’ve become addicted to the noise. What happens in an Internet minute? 16 million text messages. 1.8 million snaps. 452,000 Tweets. 156 million emails. Who has time to think, let alone write? And maybe we, in the technology industry, have shaped this reality. We created the phones, apps and 24/7-connected world. We’ve enabled society to put down the pen. The only writing I do today is email or quick WeChat posts. So now I worry that we’re losing a valuable tool that helps us to think deeply, express who we are at our greatest and expand the intellect of those around us. And for us in the technical community, this is especially troubling. The stakes are higher than ever before with AI. We’re under enormous pressure to ship quickly, to achieve more, faster, but we can’t do this at the expense of the highest engineering quality. We have to think carefully about consequences and alternatives. Who gets blamed when a self-driving car hits someone? The engineer who wrote the code is the driver. Who is accountable for the AI algorithm with bias? The engineer who created the AI. I see fewer engineers writing and sharing deep thinking, but this is what will lead to far more true innovation across the industry. How will we achieve the big transformative breakthroughs versus the incremental milestones? By writing. Because the way to think is actually to write. Putting pen to paper forces you to develop and refine your thinking by iterating, revising and exploring alternatives. Anyone who can think deeply can write beautiful code, inspiring papers or develop the plan to bring the next big thing to life. I encourage you to read Reid Hoffman’s Series B pitch for LinkedIn in which he shares the thinking that helped him succeed. At the time, he shares that a partner in a venture firm was exposed to around 5,000 pitches, looked more closely at 600 to 800, and did between 0 and 2 deals. Writing offers the possibility to create lasting artifacts. I think of papers I published that endure, albeit perhaps as reference materials. Plenoptic Sampling. Lazy Snapping. Poisson Matting. These are my work’s contribution to the field of computer vision and graphics. They will survive me and, if I’m lucky, even help shape a mind or two. One of my favorite professors at Carnegie Mellon, Takeo Kanade, said that you have to write research papers like detective novels. You need story, suspense, surprise and ‘aha’ to explain your ideas to peers, to inspire others to contribute and advance your work and the whole field. Writing is an equalizer to get the best from the whole team. At Amazon, presentations are done with the six-page paper. Meetings kick off with everyone reading followed by comments and questions to the author. Everyone operates from the same context, and introverts, extroverts and non-native speakers have an equal chance to get their thinking across. It’s not about the presenter’s personality, but the words. Ultimately, writing helps make you successful. You might be the smartest person with the best idea, but if you can’t communicate your thinking in a compelling way, you won’t get far. Two engineers in our AI+R team who inspire me with their regular writing habits are Bill Ramsey and Ronny Kohavi. Bill has written over 250 blog posts at Microsoft, benefitting our entire technical community. With Ronny, you don’t even need to meet him—his highly cited A/B test experimentation papers say it all, and he’s publishing for the benefit of the industry on LinkedIn. As you’re reading this, you may be logging your objections: I need to drive results, so I need to go straight to code. I’m known for my code, so I don’t need to write papers. I’m not a native speaker, and I speak better with my code. I don’t know what to write about. I don’t have time… But please set them aside—for your own success, for your company’s, for the industry’s advancement—and start writing. I see so many occasions for building long-form writing back into the engineering culture—planning documents, project proposals, technology LRP’s, review articles—to inspire us to work together, collectively creating and cultivating big ideas and big thinking. I took a first step recently, writing a research paper with my colleagues Xiaodong He and Di Li, From Eliza to XiaoIce: Challenges and Opportunities with Social Chatbots, for the first time in years, so please no judgment, only constructive feedback! I challenge everyone reading this piece to write 500 words per week. If you’ve got an idea or you see a problem, write your proposal and share it! Let’s rewrite our standards for thought leadership and engineering quality by writing more!","link":"/2018/03/03/you-are-what-you-write/"},{"title":"机器学习笔记-----Logistic回归","text":"Logistic回归,有翻译为“逻辑回归”，但是它跟“逻辑”这个单词一点关系都没有，也不是解决回归问题的，是用来解决二分类的算法，最准确的翻译应该是“对数几率回归” 至于为什么叫对数几率刚回归，请看下面的推倒过程 线性回归回顾先来看一个简单的线性回归模型： $$ f(x) = \\vec\\omega^T \\vec{x}+b $$这里的 $$ \\vec\\omega^T = [\\omega_1,\\omega_2,\\cdots,\\omega_n] $$ $$ \\vec x = \\left[ \\begin{matrix} x_1 \\\\ x_2 \\\\ \\cdots\\\\ x_n \\end{matrix} \\right] $$ 一般来说给定的数据集是：$D={(\\vec x^{(1)},y^{(1)}),(\\vec x^{(2)},y^{(2)}),⋯,(\\vec x^{(m)},y^{(m)})}$, 即有m组数据，n个未知数，因此多变量线性回归的公式是： $$f(x)= X^T \\vec\\omega $$ 其中： $$X = \\left[ \\begin{matrix} x_1^{(1)} \\ x_2^{(1)} \\ x_1^{(1)} \\ \\cdots \\ x_n^{(1)} \\ 1 \\\\ x_1^{(2)} \\ x_2^{(2)} \\ x_3^{(2)} \\ \\cdots \\ x_n^{(2)} \\ 1 \\\\ x_1^{(3)} \\ x_2^{(3)} \\ x_3^{(3)} \\ \\cdots \\ x_n^{(3)} \\ 1 \\\\ \\cdots \\\\ x_1^{(m)} \\ x_2^{(m)} \\ x_3^{(m)} \\ \\cdots \\ x_n^{(m)} \\ 1 \\end{matrix} \\right] = \\left[ \\begin{matrix} \\vec x^{(1)T} \\ 1 \\\\ \\vec x^{(2)T} \\ 1 \\\\ \\vec x^{(3)T} \\ 1 \\\\ \\cdots \\\\ \\vec x^{(m)T} \\ 1 \\\\ \\end{matrix} \\right]$$ 展开式： $$\\omega_1 x_1^{(1)} + \\omega_2 x_2^{(1)} + \\cdots + \\omega_n x_n^{(1)} +b = y^{(1)} $$ $$\\omega_1 x_1^{(2)} + \\omega_2 x_2^{(2)} + \\cdots + \\omega_n x_n^{(2)} + b = y^{(2)}$$ $$\\cdots$$ $$\\omega_1 x_1^{(m)} + \\omega_2 x_2^{(m)} + \\cdots + \\omega_n x_n^{(m)} +b = y^{(m)}$$ 我们为了便于推导，忽略了下标，只用了一组$(\\vec{x},y)$,然后把$b$当做$\\omega_0$，而此时$x_0 = 1$： $$f(x)=\\vec\\omega^T \\vec{x} $$ sigmoid函数线性回归很显然是一个连续的值，既然要解决二分类的问题，那最好是换成一个$0/1$的值 这里引出一个sigmoid函数：$$g(z) = \\frac {1}{1+e^{-z}}$$ 令： $$z =\\vec\\omega^T \\vec{x}+b $$ $$f(x) = g(z) = \\frac {1}{1+e^{-\\vec\\omega^T \\vec{x}}} $$ 我们就引出了我们的模型： $$ln \\frac{y}{1-y} = \\vec\\omega^T \\vec{x}+b $$ 上式中$\\frac{y}{1-y}$ 称为几率，反应x获得正例(y)的相对可能性，由此可以看出，模型实际上是在用线性回归模型的预测结果去逼近真实值$y$的对数几率，因此该模型就称为对数几率回归模型 代价函数由于$y$的值为0,1，我们将$y$视为后验概率： $$p(y=1 | x;\\omega) = f(x) $$ $$p(y=0 | x;\\omega) = 1- f(x) $$ $f(x)$越大，代表着预测值与$y$越接近，误差就越小，带入到公式中： $$p(y=1 | x;\\omega) = \\frac {e^{\\vec\\omega^T \\vec{x}}}{1+e^{\\vec\\omega^T \\vec{x}}} $$ $$p(y=0 | x;\\omega) = \\frac {1}{1+e^{\\vec\\omega^T \\vec{x}}} $$ 为了确定$\\omega$用对数似然函数： $$ \\ell(\\omega) = lnp(y|x ;\\omega)$$ 可以合并到一起： $$ p(y|x ;\\omega) = f(x)^y(1-f(x))^{(1-y)}$$ 带入到对数似然函数中： $$ln p(y|x ;\\omega) = y lnf(x) + (1-y)ln(1-f(x)) $$ 我们得到的代价函数： $$J(\\omega) = - \\frac{1}{m} \\sum_{i=1}^m [ y^{(i)} lnf(x^{(i)}) + (1-y^{(i)})ln(1-f(x^{(i)})) ] $$ $J(\\omega)$越小，就说明$f(x) = 1$的概率越大，因此只要求出$J(\\omega)$的最小值 ps： 1）其实正常推倒出来的J(w)是没有前面的负号的，这里为了习惯，用梯度下降法求最小值，所以加了一个负号 2） 之前推倒时候忽略了上标，但是我们在最后的损失函数中为了完整性就把上标加了上去 梯度下降法对代价函数求导的过程就不写了，直接写最后的结果: $$\\frac{\\partial J(\\omega)}{\\partial\\omega_j} =\\frac{1}{m} \\sum_{i=1}^m (f(x^{(i)}) - y^{(i)})x_j^{(i)} $$ $$\\omega_{j+1} := \\omega_j- \\frac{1}{m}\\alpha\\sum_{i=1}^m(f(x^{(i)}) - y^{(i)})x_j^{(i)} $$ sklearn代码实现逻辑回归1234%matplotlib inlineimport numpy as npimport matplotlib.pyplot as pltimport pandas as pd 12dataset = pd.read_csv(r'E:\\Downloads\\100-Days-Of-ML-Code-master\\Social_Network_Ads.csv')dataset.head(5) 12X = dataset.iloc[:,[2,3]].valuesY = dataset.iloc[:,4].values 12from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0) 1234from sklearn.preprocessing import StandardScalersc = StandardScaler()X_train = sc.fit_transform(X_train)X_test = sc.transform(X_test) 1234from sklearn.linear_model import LogisticRegressionclassifier = LogisticRegression()classifier.fit(X_train, y_train)# classifier.coef_ 1y_pred = classifier.predict(X_test) 12from sklearn.metrics import confusion_matrixcm = confusion_matrix(y_test, y_pred) 可视化 1234567891011121314151617181920212223242526272829303132333435from matplotlib.colors import ListedColormapX_set,y_set=X_train,y_trainX1,X2=np. meshgrid(np. arange(start=X_set[:,0].min()-1, stop=X_set[:, 0].max()+1, step=0.01), np. arange(start=X_set[:,1].min()-1, stop=X_set[:,1].max()+1, step=0.01))plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))plt.xlim(X1.min(),X1.max())plt.ylim(X2.min(),X2.max())for i,j in enumerate(np. unique(y_set)): plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1], c = ListedColormap(('red', 'green'))(i), label=j)plt. title(' LOGISTIC(Training set)')plt. xlabel(' Age')plt. ylabel(' Estimated Salary')plt. legend()plt. show()X_set,y_set=X_test,y_testX1,X2=np. meshgrid(np. arange(start=X_set[:,0].min()-1, stop=X_set[:, 0].max()+1, step=0.01), np. arange(start=X_set[:,1].min()-1, stop=X_set[:,1].max()+1, step=0.01))plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))plt.xlim(X1.min(),X1.max())plt.ylim(X2.min(),X2.max())for i,j in enumerate(np. unique(y_set)): plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1], c = ListedColormap(('red', 'green'))(i), label=j)plt. title(' LOGISTIC(Test set)')plt. xlabel(' Age')plt. ylabel(' Estimated Salary')plt. legend()plt. show()","link":"/2019/01/23/ji-qi-xue-xi-bi-ji-logistic-hui-gui/"},{"title":"机器学习笔记-----线性回归","text":"线性模型的基本形式给定由d 个属性描述的示例$\\vec x = (x_1,x_2,…x_d)$ ， 其中$x_i$是x在第i个属性上的取值，线性模型(linear model)试图学得一个通过属性的线性组合来进行预测的函数，即公式（1）：$$ f(\\vec x) = \\omega_1 x_1 + \\omega_2 x_2 +\\cdots + \\omega_d x_d+d + b $$ 写成向量式，公式（2）：$$ f(\\vec x) = \\vec\\omega^T \\vec x $$ 这里的 $$\\vec\\omega^T = [\\omega_1,\\omega_2,\\cdots,\\omega_n]$$ $$\\vec x = \\left[ \\begin{matrix} x_1 \\\\ x_2 \\\\ \\cdots\\\\ x_{n-1}\\\\ 1 \\end{matrix} \\right]$$ 式(2)是单个样本的模型，如果是多个样本，那么: $$ X = \\left[ \\begin{matrix} x_1^{(1)} \\ x_2^{(1)} \\ x_1^{(1)} \\ \\cdots \\ x_{n-1}^{(1)} \\ 1 \\\\ x_1^{(2)} \\ x_2^{(2)} \\ x_3^{(2)} \\ \\cdots \\ x_{n-1}^{(2)} \\ 1 \\\\ x_1^{(3)} \\ x_2^{(3)} \\ x_3^{(3)} \\ \\cdots \\ x_{n-1}^{(3)} \\ 1 \\\\ \\cdots \\\\ x_1^{(m)} \\ x_2^{(m)} \\ x_3^{(m)} \\ \\cdots \\ x_{n-1}^{(m)} \\ 1 \\end{matrix} \\right] = \\left[ \\begin{matrix} \\vec x^{(1)T} \\ 1 \\\\ \\vec x^{(2)T} \\ 1 \\\\ \\vec x^{(3)T} \\ 1 \\\\ \\cdots \\\\ \\vec x^{(m)T} \\ 1 \\\\ \\end{matrix} \\right] $$ 因此多变量线性回归的公式就变成： $$f(x)= X^T \\vec\\omega $$ 线性回归原理给定数据集$D = {(\\vec x_1 ,y_1),(\\vec x_2,y_2),\\cdots,(\\vec x_m,y_m)}$,线性回归就是使用$\\vec x$得到一个线性模型是的输出$y’$ 无限接近与$y$，即： 公式（3）$$f(\\vec x ) = \\vec\\omega^T \\vec x \\simeq y $$这里忽略了下标，即每一个输入$\\vec x$ 通过线性回归模型的式子$(3)$得到的$y$ 都无限接近与数据集中的$y$ 代价函数那么如何确定公式$(3)$中的$\\vec\\omega$呢? 如果我们的$f(x)$ 与$y$的差别足够小，那么我们就可以带入到公式$(3)$中直接解出$\\vec\\omega$和$b$来: $$min\\sum|f(x)-y|$$ 而在实际运用过程中我们一般用平方误差来做性能度量，公式（4）： $$min \\sum(f(x)-y)^2 $$ 当然也有其他的性能度量方式，只是平方误差这个在线性回归中最常用，这个求解线性回归参数的方法也叫做最小二乘法令： 公式（5）$$J (\\omega) = \\sum(f(x)-y)^2 $$ $J(\\omega)$在这里就叫做代价函数,吴恩达老师的机器学习课程中稍做了调整： $$J(\\omega) = \\frac{1}{2m} \\sum_{i=1}^m (f(x)-y)^2 $$ 我们只需要对$J(\\omega)$求偏导就能够求得最小值，这里假设$J(\\omega)$是凸函数 代价函数最小值求最小值可以使用梯度下降法，在这之前，我们先讲如何直接解出$J(\\omega)$的最小值： 求导法 矩阵法 显然这种方法比较复杂，而且逆矩阵也不一定存在，,貌似计算量也很大 下面讲另一种求解$J(\\omega)$最小值的方式， 梯度下降法该方法的核心思想是同步更新$\\vec\\omega$中每个值： for j in: len(w):$$\\omega_j = \\omega_j -\\alpha \\sum_{i=1}^m \\frac{\\partial J(\\omega) }{\\partial \\omega_j}$$ $\\alpha$ 称作学习率,它的值太小会增加求解的时间，太大会容易造成过拟合。该方法像是一种穷举，一步一步的找到最小值。关于这种方法的实现我们将会在逻辑回归中演示，下面讲一下Scikit-Learn关于线性回归的用法。 LinearRegression参考sklearn的官方文档中文版 linear_model.LinearRegression类的成员函数 fit 以数组X和y为输入，并将线性模型的系数$\\omega$存储在其成员变量coef_ 中： 123456&gt;&gt;&gt; from sklearn import linear_model&gt;&gt;&gt; reg = linear_model.LinearRegression()&gt;&gt;&gt; reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)&gt;&gt;&gt; reg.coef_array([ 0.5, 0.5]) 然而，最小二乘的系数估计依赖于模型特征项的独立性。当特征项相关并且设计矩阵X 的列近似的线性相关时，设计矩阵便接近于一个奇异矩阵(不是满秩，列数过多，会有多个最优解，需要正则化)，因此最小二乘估计对观测点中的随机误差变得高度敏感，产生大的方差。 手动实现线性回归这里我们使用矩阵方法求参数 1234567891011121314151617181920212223242526272829303132333435363738import numpy as npimport pandas as pdimport matplotlibimport matplotlib.pyplot as pltfrom sklearn import datasetsboston = datasets.load_boston()X = boston.dataY = boston.targetX = X[Y &lt; 50.0]Y = Y[Y &lt; 50.0]from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)# print(X.shape)# print(Y.shape)# (490, 13)# (490,)# 加一列截距X_train_b = np.insert(X_train,0,values=1,axis=1)X_test_b = np.insert(X_test,0,values=1,axis=1)# print(y_train.shape)y_train_b = y_train.reshape(y_train.shape[0],1)theta = np.linalg.inv(X_train_b.T.dot(X_train_b)).dot(X_train_b.T).dot(y_train_b)print(theta)# 使用sklearnfrom sklearn import linear_modelreg = linear_model.LinearRegression()reg.fit(X,Y)print(reg.intercept_)print(reg.coef_)# print(dir(reg))","link":"/2019/05/17/ji-qi-xue-xi-bi-ji-xian-xing-hui-gui/"},{"title":"机器学习笔记-----数据预处理","text":"数据预处理是机器学习中非常重要的步骤，为了能正确的分析数据，得到最佳的机器学习算法，拿到数据后我们一般来说都需要对数据进行预处理。数据预处理包括以下几个步骤： 导入需要的库 导入数据集 处理丢失数据 解析分类数据 拆分数据集分成训练集合和测试集合 特征缩放 图片来源与github项目机器学习100天中文翻译, 喜欢的可以去点个star哦 1. 导入库123import numpy as npimport pandas as pd 2. 导入数据集1234dataset = pd.read_csv(r'E:\\Downloads\\100-Days-Of-ML-Code-master\\100-Days-Of-ML-Code-master\\datasets\\Data.csv')X = dataset.iloc[ : , :-1].values # loc 表示标签索引行数据Y = dataset.iloc[ : , -1].values # iloc 表示按照行号号索引print(Y) 1[&apos;No&apos; &apos;Yes&apos; &apos;No&apos; &apos;No&apos; &apos;Yes&apos; &apos;Yes&apos; &apos;No&apos; &apos;Yes&apos; &apos;No&apos; &apos;Yes&apos;] 1dataset 1234567891011Country Age Salary PurchasedFrance 44 72000 NoSpain 27 48000 YesGermany 30 54000 NoSpain 38 61000 NoGermany 40 YesFrance 35 58000 YesSpain 52000 NoFrance 48 79000 YesGermany 50 83000 NoFrance 37 67000 Yes 3. 处理丢失数据123456from sklearn.preprocessing import Imputer# axis=0表示按列的平均值进行填充imputer = Imputer(missing_values = \"NaN\", strategy = \"mean\", axis = 0)imputer = imputer.fit(X[ : , 1:3]) # 处理缺失值X[ : , 1:3] = imputer.transform(X[ : , 1:3]) # 转换X 上述代码使用的是样本的平均值来代替缺失值, 也可以使用中位数、众数，接口详细信息参见help(Imputer) 4. 解析分类数据为什么要独热编码？ 使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，会让特征之间的距离计算更加合理。 为什么特征向量要映射到欧式空间？ 将离散特征通过one-hot编码映射到欧式空间，是因为，在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间 关于OneHotEncoder独热编码和 LabelEncoder标签编码 12345678910from sklearn.preprocessing import LabelEncoder, OneHotEncoderlabelencoder_X = LabelEncoder() # 对与不连续的变量进行编号X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])#Creating a dummy variableonehotencoder = OneHotEncoder(categorical_features = [0]) # 进行虚位掩码X = onehotencoder.fit_transform(X).toarray()labelencoder_Y = LabelEncoder()Y = labelencoder_Y.fit_transform(Y)print(Y)print(X[:,0]) 12[0 1 0 0 1 1 0 1 0 1][0. 1. 1. 1. 1. 0. 1. 0. 1. 0.] 5. 拆分数据集分成训练集合和测试集合123# 训练集和测试集from sklearn.model_selection import train_test_splitX_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0) 关于怎么拆分数据集 拆分数据的目的是对模型进行评估，如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分成三部分，分别为训练集（training set）、验证集（validation set）和测试集（test set）, 比例一般是 6:2:2 但是，在许多实际应用中数据是不充足的。为了选择好的模型，可以采用交叉验证方法。 交叉验证的基本想法是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。一般有 简单交叉验证， S折交叉验证， 留一交叉验证等方法 我们会在《模型评估与模型选择》一节在进行详细描述 6. 特征缩放12345# 标准化from sklearn.preprocessing import StandardScalersc_X = StandardScaler()X_train = sc_X.fit_transform(X_train) # fit_transform 先训练数据得到方差期望等信息，再进行缩放X_test = sc_X.transform(X_test) # transform直接进行数据缩放，用的是X_train的方差和期望 关于特征缩放 特征缩放就是将所有的特征分布转成成同一数量级，示例数据中的年龄与薪水的方差、均值等分布特征相差很大，这样会导致年龄对模型的影响会很小，其实就是均一化。 sklearn特征缩放StandardScaler 的计算公式: $$ x_{stand} = \\frac {x-mean(x)} {std} $$ 特征缩放的更多介绍:关于使用sklearn进行数据预处理 —— 归一化/标准化/正则化 参考资料 机器学习A-Z～数据预处理 机器学习100天","link":"/2019/05/17/ji-qi-xue-xi-bi-ji-shu-ju-yu-chu-li/"},{"title":"生新基础知识-----sam-bam文件格式详解","text":"SAM是一种序列比对格式标准，由sanger制定，是以TAB为分割符的文本格式。 主要应用于测序序列mapping到基因组上的结果表示，当然也可以表示任意的多重比对结果。 SAM的全称是sequence alignment/map format 定义和示例SAM分为两部分，注释信息（header section ）和比对结果部分 （alignment section）。 通常是把FASTQ文件格式的测序数据比对到对应的参考基因组版本得到的。 注释信息并不是SAM文件的重点，是该SAM文件产生以及被处理过程的一个记录，规定以@开头，用不同的tag表示不同的信息，主要有： @HD，说明符合标准的版本、对比序列的排列顺序； @SQ，参考序列说明； @RG，比对上的序列（read）说明； @PG，使用的程序说明； @CO，任意的说明信息。一个简单的SAM文件例子如下： 1ST-E00317:118:HNHK2CCXX:7:1101:5071:1309 97 chr1 196695666 60 147M chr18 54806715 0 AAGAATATGGACACAGTGAAGTGGTGGAATATTATTGCAATCCTAGATTTCTAATGAAGGGACCTAATAAAATTCAGTGTGTTGATGGAGAGTGGACAACTTTACCAGTGTGTATTTGTAATGTATAAAACATTAATATTGAAACTT FFAKKKKKKFKKKKKK,FKKKKKKK&lt;KKKFKK,FKKKKKKKKKFFKKKKKFFKKFAKFK,FKKKAKKKKKKKKKKKAF,,,FFAFA,FKKAKAA7FKKFKFFFKKKK7FFKK7F7K,7FAFKAFFKKKKK,7AFKFKFA,,,AAK,A MC:Z:13S134M MD:Z:76A39G13T9T6 RG:Z:sampleID NM:i:4 AS:i:127 XS:i:21 说明如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354QNAME ST-E00317:118:HNHK2CCXX:7:1101:5071:1309 ## read名称FLAG 97 ## FLAG, 概括出一个合适的标记，各个数字分别代表##1 序列是一对序列中的一个##2 比对结果是一个pair-end比对的末端##4 没有找到位点##8 这个序列是pair中的一个但是没有找到位点##16 在这个比对上的位点，序列与参考序列反向互补##32 这个序列在pair-end中的的mate序列与参考序列反响互补##64 序列是 mate 1##128 序列是 mate 2##假如说标记为以上列举出的数目，就可以直接推断出匹配的情况。假如说标记不是以上列举出的数字，比如说83=（64+16+2+1），就是这几种情况值和。## 其他情况可参见 http://broadinstitute.github.io/picard/explain-flags.htmlRNAME chr1 ## 染色体名称POS 196695666 ## 染色体位置MAQ 60 ## mapping质量 越高说明位点越独特CIGAR 147M ## “M”表示 match或 mismatch；## “I”表示 insert；## “D”表示 deletion；## “N”表示 skipped（跳过这段区域）；## “S”表示 soft clipping（被剪切的序列存在于序列中）；## “H”表示 hard clipping（被剪切的序列不存在于序列中）；## “P”表示 padding；## “=”表示 match；## “X”表示 mismatch（错配，位置是一一对应的）；MRNM chr18## read2 在参考序列上的位置，如果没有就用 &quot;*&quot;, 相同&quot;=&quot;MPOS 0 ##read2 的位置，若不可用则用0ISIZE 0## 插入片段的长度，最左边得为正，最右边的为负，中间的不用定义正负，不分区段（single-segment)的比对上，或者不可用时，此处为0SEQ AAGAATATGGACACAGTGAAGTGGTGGAATATTATTGCAATCCTAGATTTCTAATGAAGGGACCTAATAAAATTCAGTGTGTTGATGGAGAGTGGACAACTTTACCAGTGTGTATTTGTAATGTATAAAACATTAATATTGAAACTT## 序列片段的序列信息QUAL FFAKKKKKKFKKKKKK,FKKKKKKK&lt;KKKFKK,FKKKKKKKKKFFKKKKKFFKKFAKFK,FKKKAKKKKKKKKKKKAF,,,FFAFA,FKKAKAA7FKKFKFFFKKKK7FFKK7F7K,7FAFKAFFKKKKK,7AFKFKFA,,,AAK,A## 序列的质量信息，格式同FASTQ一样。read质量的ASCII编码MC:Z:13S134M MD:Z:76A39G13T9T6 RG:Z:sampleID NM:i:4 AS:i:127 XS:i:21## 可选字段（optional fields)，格式如：TAG:TYPE:VALUE，其中TAG有两个大写字母组成，每个TAG代表一类信息，每一行一个TAG只能出现一次，TYPE表示TAG对应值的类型，可以是字符串、整数、字节、数组等。## AS:i 最佳匹配的得分## XS:i 第二好的匹配的得分## YS:i mate 序列匹配的得分## XN:i 在参考序列上模糊碱基的个数## XM:i 错配的个数## XO:i gap open的个数## XG:i gap 延伸的个数## NM:i 经过编辑的序列 ## YF:i 说明为什么这个序列被过滤的字符串## YT:Z## MD:Z 代表序列和参考序列错配的字符串## 参考链接： SAM(file format)(可下载PDF文件) https://genome.sph.umich.edu/wiki/SAM#What_are_TAGs.3F https://mp.weixin.qq.com/s/yK1OyJHrePg6bWl41JCpvA http://www.cnblogs.com/emanlee/p/5366610.html","link":"/2019/05/18/sheng-xin-ji-chu-zhi-shi-sam-bam-wen-jian-ge-shi/"},{"title":"生新基础知识-----fastq文件格式详解","text":"FASTQ格式是一种保存生物序列（通常为核酸序列）及其测序质量得分信息的文本格式。序列与质量得分皆由单个ASCII字符表示。 格式 FASTQ格式通常每个序列使用四行： 第一行以“@”字符开头，后面是序列标识符和其他描述。 第二行是序列 第三行以“+”也是序列标识符和描述（可选） 第四行是序列的质量得分信息，与第二行的碱基一一对应 1234@A00262:122:H5FW3DSXX:3:1101:1561:1031 1:N:0:CCGTGAGACNACCCCAAAAATGCTTTTGAAATCCTGAGATGTGATCAGTGAAATATGCAGCCAAGGCAAGGGGAAACTGTCCGCAAGTTAAAAAGATTTATTGCTATTCCAGGCTTCAAATGAGCCCAGAACTCAGGGCTGGTGTGTGTTTCAGAAGT+F#FFFFFFFFFFFFFFFFFFFFFFFFFFFFF,F:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFF:FFFFFFFFFFFFFFFFFFF:FFFFFFFFFF:FFFFFFFFFFF:FF,FFFFF:FFFFFFF Illumina 测序仪标识符 A00262 测序仪编号 122 运行id H5FW3DSXX flowcell id 3 lane 编号 1101 tile编号 1561 tile x坐标 1031 tile y坐标 1 单端测序为1，双端为2 N Y过滤reads(reads质量较差),否则为N ps: 每个flowcell有8个泳道，一个泳道称为一个Lane，每条Lane上有两列，每列有多个有小格子，叫一个tile。(见下图) 质量评分 第四行表示序列的质量值,用ACSII码表示。测序仪一般是按照荧光信号来判断所测序的碱基是哪一种的，例如红黄蓝绿分别对应ATCG，因此对每个结果的判断都是一个概率的问题： Phred Quality Score(Q值) 错误率 碱基准确率 10 0.1 90% 20 0.01 99% 30 0.001 99.9% 40 0.0001 99.99% 50 0.00001 99.999% 一般都是以Q值来衡量read碱基质量，Sanger中心用的换算公式如下，其中P为错误率， $$Q=-10logP $$ Solexa系列测序仪使用不同的公示来计算质量值：$Q=-10log(P/1-P)$不同的测序平台Q值所能表示的范围不一样，因此要想用对应的ACSII编码，必须加上一个数值(33或者64)以上面的那条Illumina测序仪产生的read为例，F对应的十进制数是70，Q值就是36，也就是说这个碱基的准确率在99.99%以上 转载请注明出处 参考链接: http://boyun.sh.cn/bio/?p=1901 https://en.wikipedia.org/wiki/FASTQ_format https://blog.csdn.net/godsunshine/article/details/51946314","link":"/2019/05/18/sheng-xin-ji-chu-zhi-shi-fastq-wen-jian-ge-shi/"}],"tags":[{"name":"awk","slug":"awk","link":"/tags/awk/"},{"name":"摘抄","slug":"摘抄","link":"/tags/摘抄/"},{"name":"机器学习","slug":"机器学习","link":"/tags/机器学习/"},{"name":"生信基础","slug":"生信基础","link":"/tags/生信基础/"}],"categories":[{"name":"计算机","slug":"计算机","link":"/categories/计算机/"},{"name":"阅读","slug":"阅读","link":"/categories/阅读/"},{"name":"数学与统计","slug":"数学与统计","link":"/categories/数学与统计/"},{"name":"生信","slug":"生信","link":"/categories/生信/"}]}