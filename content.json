{"pages":[{"title":"关于我","text":"github: https://github.com/JeWeizhong email: zhmqsnq90@foxmail.com QQ: 584728914 wechat: 商业转载请联系作者获得授权, 非商业转载请注明出处","link":"/about/index.html"},{"title":"friends","text":"","link":"/friends/index.html"},{"title":"archives","text":"","link":"/archives/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"我为什么要写博客","text":"You Are What You Write原文：https://www.linkedin.com/pulse/you-what-write-harry-shum?from=timeline&amp;isappinstalled=0 Are Twitter, PowerPoint, Facebook, Instagram and texting eroding our ability to think? There is a Chinese proverb that says “见文如见人,” which literally means “reading the document is the same as seeing the author.” If we are what we write, then who have we, as a society, become? I was sitting in a technical review recently, listening to one of our reviewers grill the engineer who was presenting: Why did you choose that design? Why is the service showing bad results? How many users will switch to the solution? The presenter’s answers lacked depth. It seemed like he hadn’t done enough rigorous thinking, the kind where you sit quietly, sift through research, contemplate options, determine what you know, don’t know and where more work is required. The kind of thinking I did as a young researcher when peers took me and my work apart when I took short cuts. Back then, I practiced a disciplined approach, spending hours just thinking, and even more hours on the hardest part—writing it down. Today, long-form writing is being replaced. Tweets pass for dialogue. PowerPoint condenses thoughts to bullets. Words have been traded for emojis and GIFs. And we’ve become addicted to the noise. What happens in an Internet minute? 16 million text messages. 1.8 million snaps. 452,000 Tweets. 156 million emails. Who has time to think, let alone write? And maybe we, in the technology industry, have shaped this reality. We created the phones, apps and 24/7-connected world. We’ve enabled society to put down the pen. The only writing I do today is email or quick WeChat posts. So now I worry that we’re losing a valuable tool that helps us to think deeply, express who we are at our greatest and expand the intellect of those around us. And for us in the technical community, this is especially troubling. The stakes are higher than ever before with AI. We’re under enormous pressure to ship quickly, to achieve more, faster, but we can’t do this at the expense of the highest engineering quality. We have to think carefully about consequences and alternatives. Who gets blamed when a self-driving car hits someone? The engineer who wrote the code is the driver. Who is accountable for the AI algorithm with bias? The engineer who created the AI. I see fewer engineers writing and sharing deep thinking, but this is what will lead to far more true innovation across the industry. How will we achieve the big transformative breakthroughs versus the incremental milestones? By writing. Because the way to think is actually to write. Putting pen to paper forces you to develop and refine your thinking by iterating, revising and exploring alternatives. Anyone who can think deeply can write beautiful code, inspiring papers or develop the plan to bring the next big thing to life. I encourage you to read Reid Hoffman’s Series B pitch for LinkedIn in which he shares the thinking that helped him succeed. At the time, he shares that a partner in a venture firm was exposed to around 5,000 pitches, looked more closely at 600 to 800, and did between 0 and 2 deals. Writing offers the possibility to create lasting artifacts. I think of papers I published that endure, albeit perhaps as reference materials. Plenoptic Sampling. Lazy Snapping. Poisson Matting. These are my work’s contribution to the field of computer vision and graphics. They will survive me and, if I’m lucky, even help shape a mind or two. One of my favorite professors at Carnegie Mellon, Takeo Kanade, said that you have to write research papers like detective novels. You need story, suspense, surprise and ‘aha’ to explain your ideas to peers, to inspire others to contribute and advance your work and the whole field. Writing is an equalizer to get the best from the whole team. At Amazon, presentations are done with the six-page paper. Meetings kick off with everyone reading followed by comments and questions to the author. Everyone operates from the same context, and introverts, extroverts and non-native speakers have an equal chance to get their thinking across. It’s not about the presenter’s personality, but the words. Ultimately, writing helps make you successful. You might be the smartest person with the best idea, but if you can’t communicate your thinking in a compelling way, you won’t get far. Two engineers in our AI+R team who inspire me with their regular writing habits are Bill Ramsey and Ronny Kohavi. Bill has written over 250 blog posts at Microsoft, benefitting our entire technical community. With Ronny, you don’t even need to meet him—his highly cited A/B test experimentation papers say it all, and he’s publishing for the benefit of the industry on LinkedIn. As you’re reading this, you may be logging your objections: I need to drive results, so I need to go straight to code. I’m known for my code, so I don’t need to write papers. I’m not a native speaker, and I speak better with my code. I don’t know what to write about. I don’t have time… But please set them aside—for your own success, for your company’s, for the industry’s advancement—and start writing. I see so many occasions for building long-form writing back into the engineering culture—planning documents, project proposals, technology LRP’s, review articles—to inspire us to work together, collectively creating and cultivating big ideas and big thinking. I took a first step recently, writing a research paper with my colleagues Xiaodong He and Di Li, From Eliza to XiaoIce: Challenges and Opportunities with Social Chatbots, for the first time in years, so please no judgment, only constructive feedback! I challenge everyone reading this piece to write 500 words per week. If you’ve got an idea or you see a problem, write your proposal and share it! Let’s rewrite our standards for thought leadership and engineering quality by writing more!","link":"/2018/03/03/you-are-what-you-write/"},{"title":"AWK简明教程","text":"AWK是一种处理文本文件的语言，是一个强大的文本分析工具。之所以叫AWK是因为其取了三位创始人 Alfred Aho，Peter Weinberger, 和 Brian Kernighan 的 Family Name 的首字符。 awk命令格式和选项语法形式 12awk [options] &apos;script&apos; var=value file(s)awk [options] -f scriptfile var=value file(s) 常用命令选项 -F fs fs指定输入分隔符，fs可以是字符串或正则表达式，如-F: -v var=value 赋值一个用户定义变量，将外部变量传递给awk -f scripfile 从脚本文件中读取awk命令 -m[fr] val 对val值设置内在限制，-mf选项限制分配给val的最大块数目；-mr选项限制记录的最大数目。这两个功能是Bell实验室版awk的扩展功能，在标准awk中不适用。 工作原理 1hawk &apos;BEGIN{ commands } pattern{ commands } END{ commands }&apos; 内建变量 变量 详细说明 $0 当前记录（这个变量中存放着整个行的内容） $1~$n 当前记录的第n个字段，字段间由FS分隔 FS 输入字段分隔符 默认是空格或Tab (也可以是-F) NF 当前记录中的字段个数，就是有多少列 NR 已经读出的记录数，就是行号，从1开始，如果有多个文件话，这个值也是不断累加中。 FNR 当前记录数，与NR不同的是，这个值会是各个文件自己的行号 RS 输入的记录分隔符， 默认为换行符 OFS 输出字段分隔符， 默认也是空格 ORS 输出的记录分隔符，默认为换行符 FILENAME 当前输入文件的名字 模式匹配1$ awk '$6 ~ /FIN|TIME/ || NR==1 {print NR,$4,$5,$6}' OFS=\"\\t\" netstat.txt 统计下面的命令计算所有的C文件，CPP文件和H文件的文件大小总和。 12$ ls -l *.cpp *.c *.h | awk &apos;{sum+=$5} END {print sum}&apos;2511401 统计每个用户的进程的占了多少内存 123456$ ps aux | awk 'NR!=1{a[$1]+=$6;} END { for(i in a) print i \", \" a[i]\"KB\";}'dbus, 540KBmysql, 99928KBwww, 3264924KBroot, 63644KBhchen, 6020KB 控制结构和脚本语言待补充 参考链接： http://man.linuxde.net/awkhttps://coolshell.cn/articles/9070.html","link":"/2018/05/16/awk-jian-ming-jiao-cheng/"},{"title":"python3 标准库系列(三)---os.path","text":"os.path模块比较简单，先记住下面这些常用的用就可以了 1234567891011121314151617181920212223242526272829303132333435363738394041424344&gt;&gt;&gt; import os&gt;&gt;&gt; path = '/Users/beazley/Data/data.csv'&gt;&gt;&gt; # 获取文件名&gt;&gt;&gt; os.path.basename(path)'data.csv'&gt;&gt;&gt; # 获取文件所在目录名&gt;&gt;&gt; os.path.dirname(path)'/Users/beazley/Data'&gt;&gt;&gt; # 多个元素拼接成一个链接&gt;&gt;&gt; os.path.join('tmp', 'data', os.path.basename(path))'tmp/data/data.csv'&gt;&gt;&gt; # 将家目录符号~换成真实链接&gt;&gt;&gt; path = '~/Data/data.csv'&gt;&gt;&gt; os.path.expanduser(path)'/Users/beazley/Data/data.csv'&gt;&gt;&gt; # 获取文件扩展名&gt;&gt;&gt; os.path.splitext(path)('~/Data/data', '.csv')&gt;&gt;&gt;# 检查文件或目录是否存在&gt;&gt;&gt; os.path.exists('/etc/passwd')True&gt;&gt;&gt; # 检测是不是文件类型&gt;&gt;&gt; os.path.isfile('/etc/passwd')True&gt;&gt;&gt; # 检测是不是目录类型&gt;&gt;&gt; os.path.isdir('/etc/passwd')False&gt;&gt;&gt; # 检测是不是符号链接&gt;&gt;&gt; os.path.islink('/usr/local/bin/python3')True&gt;&gt;&gt; # 返回一个绝对路径&gt;&gt;&gt; os.path.realpath('/usr/local/bin/python3')'/usr/local/bin/python3.3'&gt;&gt;&gt;# 获取文件大小&gt;&gt;&gt; os.path.getsize('/etc/passwd')3669&gt;&gt;&gt; # 获取文件修改日期&gt;&gt;&gt; os.path.getmtime('/etc/passwd')1272478234.0&gt;&gt;&gt; import time&gt;&gt;&gt; time.ctime(os.path.getmtime('/etc/passwd'))'Wed Apr 28 13:10:34 2010'&gt;&gt;&gt; # 获取指定目录下的文件列表 &gt;&gt;&gt;os.listdir('somedir')","link":"/2018/11/07/python3-biao-zhun-ku-xi-lie-san-os.path/"},{"title":"python3 标准库系列(六)---base64","text":"base64模块是一种用64个字符来表示任意二进制数据的方法。 123import base64# 将bytes字转为64位 存储base64.b64encode(b'hello world') b&apos;aGVsbG8gd29ybGQ=&apos;1base64.b64decode(b'aGVsbG8gd29ybGQ=') # 解码 b&apos;hello world&apos;1base64.b64decode(b'aGVsbG8gd29ybGQ=').decode(\"utf-8\") # 解码后以utf-8编码 &apos;hello world&apos;","link":"/2018/11/30/python3-biao-zhun-ku-xi-lie-liu-base64/"},{"title":"python3 标准库系列(五)---subprocess","text":"subprocess模块是用来执行外部命令，下面介绍两种主要的用法。 subprocess.run 这里call, check_call, check_output, 与其功能性相似，也都是会造成进程阻塞，就不展开介绍了 常用参数： 123456789101112subprocess.run (args，*, stdin = None, input = None, stdout = None, stderr = None, shell = False, cwd = None, timeout = None, check = False, encoding = None, errors = None, env = None ) 默认不会捕获stdout或stderr 如果check为true，并且进程以非零退出代码退出， CalledProcessError则会引发异常。该异常的属性包含参数，退出代码以及stdout和stderr（如果它们被捕获）, 1234567# 这里捕获了stdout,stderr&gt;&gt;&gt; a = subprocess.run([\"ls\", \"-l\", \"/dev/null\"],stdout=subprocess.PIPE,stderr=subprocess.PIPE)&gt;&gt;&gt; a.stdoutb'crw-rw-rw-. 1 root root 1, 3 8\\xe6\\x9c\\x88 3 13:24 /dev/null\\n'&gt;&gt;&gt; a.stdoutb'crw-rw-rw-. 1 root root 1, 3 8\\xe6\\x9c\\x88 3 13:24 /dev/null\\n'&gt;&gt;&gt; a.stdout.decode(\"utf-8\") 如果我们使用了check参数的话，如果执行错误，就会引发异常，这里什么都捕获不到 12345678910&gt;&gt;&gt; b = subprocess.run([\"ls\", \"-l\", \"/null\"], check=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/share/chg2master/prod/Other/zhongjw/miniconda3/lib/python3.6/subprocess.py\", line 418, in run output=stdout, stderr=stderr)subprocess.CalledProcessError: Command '['ls', '-l', '/null']' returned non-zero exit status 2.&gt;&gt;&gt; dir(b)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;NameError: name 'b' is not defined python3.7里新增了capture_output参数,如果设置为true，则将捕获stdout和stderr。 使用时，将使用stdout = PIPE和stderr = PIPE自动创建内部Popen对象。 也可能不使用stdout和stderr参数。那么Popen对象的stdout、stderr与上面示例里的有什么不同呢？ subprocess.Popensubprocess中更底层的进程创建和管理可以通过Popen类实现。它提供了更多的灵活性，程序员通过它能处理更多复杂的情况。 常用参数： 12345678910111213141516171819subprocess.Popen(args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None, universal_newlines=False, startupinfo=None, creationflags=0, restore_signals=True, start_new_session=False, pass_fds=(), *, encoding=None, errors=None) 示例： 1234&gt;&gt;&gt; obj = subprocess.Popen([\"ls\", \"-l\", \"/null\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)&gt;&gt;&gt; obj.stderr.read().decode(\"utf-8\")'ls: 无法访问/null: 没有那个文件或目录\\n'&gt;&gt;&gt; obj.stderr.read().decode(\"utf-8\") 从上面示例中可以看出Popen对象的stdout、stderr获取的是数据流，而run捕获的stdout、stderr是字符串格式 当然官方文档说是不建议使用.stdin.write,.stdout.read 或者 .stderr.read，而是用.communicate方法，该方法会造成阻塞，以防止其他进程读取数据流造成死锁 .communicate()方法如果同时有.stdout和 .stderr, 返回的是一个元组, 用下标可以访问捕获的stdout和 stderr。 还要补充一点就是如果不捕获输出的话，是不会造成阻塞的，如果要等待子进程运行完可以使用.wait方法，而subprocesee.run方法会造成主程序的阻塞，因为要获取子进程运行状态。 Popen对象的主要方法有： 1['communicate', 'kill', 'pid', 'poll', 'returncode', 'send_signal', 'stderr', 'stdin', 'stdout', 'terminate', 'universal_newlines', 'wait'] 如果有需要使用其他的参数和方法请查阅官方文档","link":"/2018/11/14/python3-biao-zhun-ku-xi-lie-wu-subprocess/"},{"title":"python3 标准库系列(一)---email模块","text":"自己写的一个发邮件的类，自己用的时候，省的再到处翻教程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108#-*-coding:utf-8-*-import smtplibimport sysimport reimport osfrom email.mime.multipart import MIMEMultipartfrom email.mime.text import MIMETextfrom email.mime.image import MIMEImagefrom email import encodersfrom email.header import Headerfrom email.utils import parseaddr, formataddr'''发送邮件：文本，网页，附件，图片'''class Sendemail(): def __init__(self,mailToList,mailHost,mailUser,mailPass,sub): self.mailToList = mailToList # 邮件接收方的邮件地址，传入的必须是list类型 self.mailHost = mailHost # 邮件传送协议服务器: smtp.qiye.163.com self.mailUser = mailUser # 邮件发送方的邮箱账号: xxxx@xxxx.org self.mailPass = mailPass # 邮件发送方的邮箱密码: **** self.sub = sub # 邮件名 #self.content = content # 内容 #self.subtype = subtype # 发送类型 plain(文字) def _formatAddr(self,s): ''' 处理发件人的名字，防止有带中文名的发件人 ''' name, addr = parseaddr(s) return formataddr((Header(name, 'utf-8').encode(), addr)) def sendEmail(self,content,subtype): msg = MIMEMultipart('mixed') # 默认就是 mixed类型 me = re.split('@',self.mailUser)[0]+\"&lt;\"+self.mailUser+\"&gt;\" msg['Subject'] = self.sub msg['From'] = self._formatAddr(me) msg['To'] = ','.join(self.mailToList) # 多个收件人以“,”分隔 text = MIMEText(content, _subtype=subtype, _charset='utf-8') msg.attach(text) try: server = smtplib.SMTP() server.connect(self.mailHost) server.login(self.mailUser, self.mailPass) server.sendmail(me, self.mailToList, msg.as_string()) server.quit() print(\"发送成功\") return True except Exception as e: print (e) return False def sendFile(self,file): ''' 发送附件''' msg = MIMEMultipart('mixed') # 默认就是 mixed类型 me = re.split('@',self.mailUser)[0]+\"&lt;\"+self.mailUser+\"&gt;\" msg['Subject'] = self.sub msg['From'] = self._formatAddr(me) msg['To'] = ','.join(self.mailToList) sendfile=open(file,'rb').read() text_att = MIMEText(sendfile, 'base64', 'utf-8') text_att[\"Content-Type\"] = 'application/octet-stream' # 设置附件名字，如果是windows下的中文文件名需要以gbk编码，这里默认是再Linux上的中文名字的文件 text_att.add_header('Content-Disposition', 'attachment', filename=(\"utf-8\", \"\",os.path.basename(file))) #return text_att msg.attach(text_att) try: server = smtplib.SMTP() server.connect(self.mailHost) server.login(self.mailUser, self.mailPass) server.sendmail(me, self.mailToList, msg.as_string()) server.quit() print(\"发送成功\") return True except Exception as e: print (e) return False def main(): mailToList = [\"xxx@xx.com\",\"xxx@xxx.com\"] # 邮件接收方的邮件地址，传入的必须是list类型 mailHost = \"smtp.qiye.163.com\" # 邮件传送协议服务器: smtp.qiye.163.com mailUser = \"xxx@xxx.com\" # 邮件发送方的邮箱账号: xxxx@xxxx.com mailPass = \"xxxx\" # 邮件发送方的邮箱密码: **** sub = \"测试邮件\" # 邮件名 s = Sendemail(mailToList,mailHost,mailUser,mailPass,sub) #content 内容/文件路径 #subtype 发送类型 plain(文字) html(网页) 正文图片(目前只能实现插入一张图) 附件 content = \"test\" subtype = \"plain\" # 如果发送的是附件用这个函数，暂时不支持邮件正文中发图片，因为比较麻烦，就是把图片插入网页格式中 s.sendFile(\"filepath\")if __name__ == '__main__': main()","link":"/2018/11/02/python3-biao-zhun-ku-xi-lie-yi-email/"},{"title":"python3 标准库系列(二)---argparse 模块","text":"argparse 模块是标准库中最大的模块之一，拥有大量的配置选项，本章只列出了一些常用的选项 1234567891011121314151617181920import argparseparser = argparse.ArgumentParser(description='Search some files')parser.add_argument(dest='filenames',metavar='filename', nargs='*')parser.add_argument('-p', '--pat',metavar='pattern', required=True,dest='patterns', action='append', help='text pattern to search for')parser.add_argument('-v', dest='verbose', action='store_true',help='verbose mode')parser.add_argument('-o', dest='outfile', action='store',help='output file')parser.add_argument('--speed', dest='speed', action='store',choices={'slow','fast'}, default='slow',help='search speed')args = parser.parse_args()# 使用方法print(args.filenames)print(args.patterns)print(args.verbose)print(args.outfile)print(args.speed) 为了解析命令行选项， 你首先要创建一个ArgumentParser 实例，并使用add_argument()方法声明你想要支持的选项。 在每个add_argument() 调用中， dest参数指定解析结果被指派给属性的名字。 metavar 参数被用来生成帮助信息(metavar仅更改显示的名称，parse_args()对象上属性的名称仍由dest值确定)。 action参数指定跟属性对应的处理逻辑，通常的值为store , 被用来存储某个值或将多个参数值收集到一个列表中。其他参数：store_true,store_false: 123456&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument('--foo', action='store_true')&gt;&gt;&gt; parser.add_argument('--bar', action='store_false')&gt;&gt;&gt; parser.add_argument('--baz', action='store_false')&gt;&gt;&gt; parser.parse_args('--foo --bar'.split())Namespace(foo=True, bar=False, baz=True) 下面的参数收集所有剩余的命令行参数到一个列表中。在本例中它被用来构造一个文件名列表： 1parser.add_argument(dest='filenames',metavar='filename', nargs='*') 下面的参数根据参数是否存在来设置一个Boolean 标志： 1parser.add_argument('-v', dest='verbose', action='store_true',help='verbose mode') 下面的参数接受一个单独值并将其存储为一个字符串： 1parser.add_argument('-o', dest='outfile', action='store',help='output file') 如果要接收多个值，可以使用nargs参数，其支持的值有： N (整数), 检测命令行的该参数值的个数，只接收N个值 * , 也就是说参数可以接收任意个数值 +, 与‘*’ 一样，但是如果少于一个值，会生成错误信息，有点像通配符 ？，如果传入了参数就使用参数给定的值，如果没有，就用默认值,这个需要设置默认值 argparse.REMAINDER所有剩余的命令行参数都收集到一个列表中: 123456&gt;&gt;&gt; parser = argparse.ArgumentParser(prog='PROG')&gt;&gt;&gt; parser.add_argument('--foo')&gt;&gt;&gt; parser.add_argument('command')&gt;&gt;&gt; parser.add_argument('args', nargs=argparse.REMAINDER)&gt;&gt;&gt; print(parser.parse_args('--foo B cmd --arg1 XX ZZ'.split()))Namespace(args=['--arg1', 'XX', 'ZZ'], command='cmd', foo='B') 下面的参数说明接受一个值，但是会将其和可能的选择值做比较，以检测其合法性： 1parser.add_argument('--speed', dest='speed', action='store',choices={'slow','fast'}, default='slow',help='search speed') 也就是说只能选择choices参数中的值，否则会报错 其他一些参数： default: 设置默认值 type: 设置接收的内置类型，如float或int,(常见的内置类型和函数可以直接用作type参数的值) 12345&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument('foo', type=int)&gt;&gt;&gt; parser.add_argument('bar', type=open)&gt;&gt;&gt; parser.parse_args('2 temp.txt'.split())Namespace(bar=&lt;_io.TextIOWrapper name='temp.txt' encoding='UTF-8'&gt;, foo=2) required: 设置为必须参数 ArgumentParser.add_argument_group可以参数设置分组： 12345678910&gt;&gt;&gt; parser = argparse.ArgumentParser(prog='PROG', add_help=False)&gt;&gt;&gt; group = parser.add_argument_group('group')&gt;&gt;&gt; group.add_argument('--foo', help='foo help')&gt;&gt;&gt; group.add_argument('bar', help='bar help')&gt;&gt;&gt; parser.print_help()usage: PROG [--foo FOO] bargroup: bar bar help --foo FOO foo help 1234567891011121314151617&gt;&gt;&gt; parser = argparse.ArgumentParser(prog='PROG', add_help=False)&gt;&gt;&gt; group1 = parser.add_argument_group('group1', 'group1 description')&gt;&gt;&gt; group1.add_argument('foo', help='foo help')&gt;&gt;&gt; group2 = parser.add_argument_group('group2', 'group2 description')&gt;&gt;&gt; group2.add_argument('--bar', help='bar help')&gt;&gt;&gt; parser.print_help()usage: PROG [--bar BAR] foogroup1: group1 description foo foo helpgroup2: group2 description --bar BAR bar help ArgumentParser.add_mutually_exclusive_group(required=False) 可以设置一组互斥的参数组，即在该组中参数只能选零个或一个,当required=True时，标准该参数组必须选组中的一个参数： 123456789&gt;&gt;&gt; parser = argparse.ArgumentParser(prog='PROG')&gt;&gt;&gt; group = parser.add_mutually_exclusive_group(required=True)&gt;&gt;&gt; group.add_argument('--foo', action='store_true')&gt;&gt;&gt; group.add_argument('--bar', action='store_false')&gt;&gt;&gt; parser.parse_args([]) # 如果required=False,那么这里不会报错# Namespace(bar=True, foo=False)usage: PROG [-h] (--foo | --bar)PROG: error: one of the arguments --foo --bar is required 添加一个子命令 123456789101112131415import argparseparser = argparse.ArgumentParser(prog='PROG')parser.add_argument('--foo', action='store_true', help='foo help')subparsers = parser.add_subparsers(help='sub-command help')# create the parser for the \"a\" commandparser_a = subparsers.add_parser('a', help='a help')parser_a.add_argument('bar', type=int, help='bar help')# create the parser for the \"b\" commandparser_b = subparsers.add_parser('b', help='b help')parser_b.add_argument('--baz', choices='XYZ', help='baz help')args = parser.parse_args() 1234567891011$python temp.py -husage: PROG [-h] [--foo] {a,b} ...positional arguments: {a,b} sub-command help a a help b b helpoptional arguments: -h, --help show this help message and exit --foo foo help 12345678$python temp.py a -husage: PROG a [-h] barpositional arguments: bar bar helpoptional arguments: -h, --help show this help message and exit 快速使用(自用) 123456789def _argparses(): parser = argparse.ArgumentParser(description='description') parser.add_argument('-i','--input',required=True,dest='input', help=\"input file\") parser.add_argument('-o','--output',required=True,dest='output',help=\"output file\") if len(sys.argv) == 1: parser.print_help() sys.exit() return parser.parse_args()","link":"/2018/11/07/python3-biao-zhun-ku-xi-lie-er-argparse/"},{"title":"机器学习笔记-----最小二乘法","text":"本文章转载自： 刘建平Pinard- 最小二乘法小结 最小二乘法是用来做函数拟合或者求函数极值的方法。在机器学习，尤其是回归模型中，经常可以看到最小二乘法的身影，这里就对我对最小二乘法的认知做一个小结。 1.最小二乘法的原理与要解决的问题最小二乘法是由勒让德在19世纪发现的，原理的一般形式很简单，当然发现的过程是非常艰难的。形式如下式：$$ loss = \\sum (f(x) - y )^2$$ f(x)就是我们的多组样本，y就是我们的假设拟合函数。loss也就是在机器学习中常说的损失函数，我们的目标是得到使目标函数最小化时候的拟合函数的模型。举一个最简单的线性回归的简单例子，比如我们有m个只有一个特征的样本： $$(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}),…(x^{(m)},y^{(m)})$$ 样本采用下面的拟合函数： $$h_\\theta(x) = \\theta_0 + \\theta_1 x$$ 这样我们的样本有一个特征x，对应的拟合函数有两个参数$\\theta_0 和 \\theta_1$需要求出。 我们的目标函数为： $$J(\\theta_0, \\theta_1) = \\sum\\limits_{i=1}^{m}(y^{(i)} - h_\\theta(x^{(i)})^2= \\sum\\limits_{i=1}^{m}(y^{(i)} - \\theta_0 - \\theta_1 x^{(i)})^2$$ 用最小二乘法做什么呢，使$J(\\theta_0, \\theta_1)$最小，求出使$J(\\theta_0,\\theta_1)$最小时的$\\theta_0 和 \\theta_1$，这样拟合函数就得出了。 那么，最小二乘法怎么才能使$J(\\theta_0, \\theta_1)$最小呢？ 2.最小二乘法的代数法解法上面提到要使$J(\\theta_0, \\theta_1)$最小，方法就是对$\\theta_0$和$\\theta_1$分别来求偏导数，令偏导数为0，得到一个关于$\\theta_0 和\\theta_1$的二元方程组。求解这个二元方程组，就可以得到$\\theta_0$ 和 $\\theta_1$的值。下面我们具体看看过程。 $J(\\theta_0, \\theta_1)$对$\\theta_0$求导，得到如下方程： ① $$\\sum\\limits_{i=1}^{m}(y^{(i)} - \\theta_0 - \\theta_1 x^{(i)}) = 0$$ $J(\\theta_0, \\theta_1)$对$\\theta_1$求导，得到如下方程：② $$\\sum\\limits_{i=1}^{m}(y^{(i)} - \\theta_0 - \\theta_1 x^{(i)})x^{(i)} = 0$$ ①和②组成一个二元一次方程组，容易求出$\\theta_0$ 和 $\\theta_1$的值： $$\\theta_0 = \\frac {\\sum\\limits_{i=1}^{m}\\big(x^{(i)})^2\\sum\\limits_{i=1}^{m}y^{(i)}-\\sum\\limits_{i=1}^{m}x^{(i)}\\sum\\limits_{i=1}^{m}x^{(i)}y^{(i)}}{m\\sum\\limits_{i=1}^{m}\\big(x^{(i)})^2 -\\big(\\sum\\limits_{i=1}^{m}x^{(i)})^2}$$ $$\\theta_1 = \\frac {m\\sum\\limits_{i=1}^{m}x^{(i)}y^{(i)} -\\sum\\limits_{i=1}^{m}x^{(i)}\\sum\\limits_{i=1}^{m}y^{(i)}} {m\\sum\\limits_{i=1}^{m}\\big(x^{(i)})^2 -\\big(\\sum\\limits_{i=1}^{m}x^{(i)})^2}$$ 这个方法很容易推广到多个样本特征的线性拟合。 拟合函数表示为 $h_\\theta(x_1, x_2, …x_n) = \\theta_0 + \\theta_{1}x_1 + … +\\theta_{n}x_{n}$, 其中 $\\theta_i$ (i = 0,1,2… n)为模型参数，$x_i $ (i =0,1,2… n)为每个样本的n个特征值。这个表示可以简化，我们增加一个特征$x_0 = 1$ ，这样拟合函数表示为： $$h_\\theta(x_0,x_1, …x_n) = \\sum\\limits_{i=0}^{n} \\theta_{i} x_{i}$$ 损失函数表示为： $$J(\\theta_0, \\theta_1…, \\theta_n) = \\sum\\limits_{j=1}^{m}(h_\\theta(x_0^{(j)}, x_1^{(j)}, …x_n^{(j)}) - y^{(j)})^2 = \\sum\\limits_{j=1}^{m}(\\sum\\limits_{i=0}^{n}(\\theta_{i}x_{i}^{(j)}- y^{(j)}))^2$$ 利用损失函数分别对$\\theta_i$(i=0,1,…n)求导,并令导数为0可得： $$\\sum\\limits_{j=0}^{m}(\\sum\\limits_{i=0}^{n}(\\theta_{i}x_{i}^{(j)} -y^{(j)})x_i^{(j)}) = 0 , (i=0,1,…n)$$ 这样我们得到一个N+1元一次方程组，这个方程组有N+1个方程，求解这个方程，就可以得到所有的N+1个未知的$\\theta$ 这个方法很容易推广到多个样本特征的非线性拟合。原理和上面的一样，都是用损失函数对各个参数求导取0，然后求解方程组得到参数值。这里就不累述了。 3.最小二乘法的矩阵法解法矩阵法比代数法要简洁，且矩阵运算可以取代循环，所以现在很多书和机器学习库都是用的矩阵法来做最小二乘法。 这里用上面的多元线性回归例子来描述矩阵法解法。 假设函数$h_\\theta(x_1, x_2, …x_n) = \\theta_0 + \\theta_{1}x_1 + … +\\theta_{n-1}x_{n-1}$的矩阵表达方式为： $$h_\\mathbf{\\theta}(\\mathbf{x}) = \\mathbf{X\\theta}$$ 其中，假设函数$h_\\mathbf{\\theta}(\\mathbf{X})$为mx1的向量,$\\mathbf{\\theta}$为nx1的向量，里面有n个代数法的模型参数。$\\mathbf{X}$为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。 损失函数定义为$J(\\mathbf\\theta) = \\frac{1}{2}(\\mathbf{X\\theta} -\\mathbf{Y})^T(\\mathbf{X\\theta} - \\mathbf{Y})$ 其中$\\mathbf{Y}$是样本的输出向量，维度为$mx1$. $\\frac{1}{2}$在这主要是为了求导后系数为1，方便计算。 根据最小二乘法的原理，我们要对这个损失函数对$\\mathbf{\\theta}$向量求导取0。结果如下式： $$\\frac{\\partial}{\\partial\\mathbf\\theta}J(\\mathbf\\theta) =\\mathbf{X}^T(\\mathbf{X\\theta} - \\mathbf{Y}) = 0$$ 这里面用到了矩阵求导链式法则，和两个个矩阵求导的公式。 公式1：$\\frac{\\partial}{\\partial\\mathbf{x}}(\\mathbf{x^Tx})=2\\mathbf{x}$, x为向量 公式2：$\\nabla_Xf(AX+B) = A^T\\nabla_Yf,;; Y=AX+B$, f(Y)为标量 对上述求导等式整理后可得： $$\\mathbf{X^{T}X\\theta} = \\mathbf{X^{T}Y}$$ 两边同时左乘$(\\mathbf{X^{T}X})^{-1}$可得： $$ \\mathbf{\\theta} = (\\mathbf{X^{T}X})^{-1}\\mathbf{X^{T}Y} $$ 这样我们就一下子求出了$\\theta$向量表达式的公式，免去了代数法一个个去求导的麻烦。只要给了数据,我们就可以用$\\mathbf{\\theta} = (\\mathbf{X^{T}X})^{-1}\\mathbf{X^{T}Y}$算出$\\theta$。 4.最小二乘法的局限性和适用场景从上面可以看出，最小二乘法适用简洁高效，比梯度下降这样的迭代法似乎方便很多。但是这里我们就聊聊最小二乘法的局限性。 首先，最小二乘法需要计算$\\mathbf{X^{T}X}$的逆矩阵，有可能它的逆矩阵不存在，这样就没有办法直接用最小二乘法了，此时梯度下降法仍然可以使用。当然，我们可以通过对样本数据进行整理，去掉冗余特征。让$\\mathbf{X^{T}X}$的行列式不为0，然后继续使用最小二乘法。 第二，当样本特征n非常的大的时候，计算$\\mathbf{X^{T}X}$的逆矩阵是一个非常耗时的工作（nxn的矩阵求逆），甚至不可行。此时以梯度下降为代表的迭代法仍然可以使用。那这个n到底多大就不适合最小二乘法呢？如果你没有很多的分布式大数据计算资源，建议超过10000个特征就用迭代法吧。或者通过主成分分析降低特征的维度后再用最小二乘法。 第三，如果拟合函数不是线性的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用，此时梯度下降仍然可以用。 第四，讲一些特殊情况。当样本量m很少，小于特征数n的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。当样本量m等于特征数n的时候，用方程组求解就可以了。当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。","link":"/2019/05/19/ji-qi-xue-xi-bi-ji-zui-xiao-er-cheng-fa/"},{"title":"生信基础知识-----fastq文件格式详解","text":"FASTQ格式是一种保存生物序列（通常为核酸序列）及其测序质量得分信息的文本格式。序列与质量得分皆由单个ASCII字符表示。 格式 FASTQ格式通常每个序列使用四行： 第一行以“@”字符开头，后面是序列标识符和其他描述。 第二行是序列 第三行以“+”也是序列标识符和描述（可选） 第四行是序列的质量得分信息，与第二行的碱基一一对应 1234@A00262:122:H5FW3DSXX:3:1101:1561:1031 1:N:0:CCGTGAGACNACCCCAAAAATGCTTTTGAAATCCTGAGATGTGATCAGTGAAATATGCAGCCAAGGCAAGGGGAAACTGTCCGCAAGTTAAAAAGATTTATTGCTATTCCAGGCTTCAAATGAGCCCAGAACTCAGGGCTGGTGTGTGTTTCAGAAGT+F#FFFFFFFFFFFFFFFFFFFFFFFFFFFFF,F:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFF:FFFFFFFFFFFFFFFFFFF:FFFFFFFFFF:FFFFFFFFFFF:FF,FFFFF:FFFFFFF Illumina 测序仪标识符 A00262 测序仪编号 122 运行id H5FW3DSXX flowcell id 3 lane 编号 1101 tile编号 1561 tile x坐标 1031 tile y坐标 1 单端测序为1，双端为2 N Y过滤reads(reads质量较差),否则为N ps: 每个flowcell有8个泳道，一个泳道称为一个Lane，每条Lane上有两列，每列有多个有小格子，叫一个tile。(见下图) 质量评分 第四行表示序列的质量值,用ACSII码表示。测序仪一般是按照荧光信号来判断所测序的碱基是哪一种的，例如红黄蓝绿分别对应ATCG，因此对每个结果的判断都是一个概率的问题： Phred Quality Score(Q值) 错误率 碱基准确率 10 0.1 90% 20 0.01 99% 30 0.001 99.9% 40 0.0001 99.99% 50 0.00001 99.999% 一般都是以Q值来衡量read碱基质量，Sanger中心用的换算公式如下，其中P为错误率， $$Q=-10logP $$ Solexa系列测序仪使用不同的公示来计算质量值：$Q=-10log(P/1-P)$不同的测序平台Q值所能表示的范围不一样，因此要想用对应的ACSII编码，必须加上一个数值(33或者64)以上面的那条Illumina测序仪产生的read为例，F对应的十进制数是70，Q值就是36，也就是说这个碱基的准确率在99.99%以上 转载请注明出处 参考链接: http://boyun.sh.cn/bio/?p=1901 https://en.wikipedia.org/wiki/FASTQ_format https://blog.csdn.net/godsunshine/article/details/51946314","link":"/2019/05/18/sheng-xin-ji-chu-zhi-shi-fastq-wen-jian-ge-shi/"},{"title":"机器学习笔记-----梯度下降与随机梯度下降","text":"自己写代码实现梯度下降法和随机梯度下降法 损失函数与迭代公式线性回归损失函数: $$J(\\mathbf\\theta) = \\frac{1}{2}(\\mathbf{X\\theta} -\\mathbf{Y})^T(\\mathbf{X\\theta} - \\mathbf{Y})$$ 逻辑回归损失函数： $$J(\\theta) = - \\frac{1}{m}\\sum_{i=1}^m [ y^{(i)} lnf(x^{(i)}) + (1-y^{(i)})ln(1-f(x^{(i)})) ]$$ 损失函数求导 $$\\frac{\\partial}{\\partial\\mathbf\\theta}J(\\mathbf\\theta) =\\mathbf{X}^T(\\mathbf{X\\theta} - \\mathbf{Y})$$ 其实逻辑回归的损失函数就是代入了一个sigmoid函数： $$g(\\mathbf{X\\theta}) = \\frac {1}{1+e^{\\mathbf{X\\theta}}}$$ 迭代公式： $$\\omega_{j} := \\omega_j- \\frac{1}{m}\\alpha\\sum_{i=1}^m(f(x^{(i)}) - y^{(i)})x_j^{(i)} $$ 如果是随机梯度下降，那么这里m=1 或者： $$\\theta{j} := \\theta_j- \\frac{1}{m} \\alpha \\mathbf {X}^T (g(\\mathbf{X}\\theta) - \\vec y) $$ 注：$\\frac1m$也可以不加 梯度下降12345# 1. 导入需要的库import pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScaler 1234567891011# 数据来源: http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/# 下载数据: wget http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data# 2. 导入数据集#'样品代码编号'，'团块厚度'，'细胞大小均匀性'， '细胞形状的均匀性'，'边缘粘附'，'单个上皮细胞大小'，...'种类'column_names=['Sample code number','Clump Thickness','Uniformity of Cell Size', 'Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size', 'Bare nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','Class']data=pd.read_csv(r'http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data',names=column_names)print(data.shape) 1(699, 11) 1234567891011# 3. 处理丢失数据from sklearn.preprocessing import Imputer# axis=0表示按列的平均值进行填充# imputer = Imputer(missing_values = r\"?\", strategy = \"mean\", axis = 0)# imputer = imputer.fit(data) # 处理缺失值# data = imputer.transform(data) # 转换# Imputer 的接口只能处理整数和\"NaN\" 所以我们用pandas内置的处理缺失值的方法data = data.replace(to_replace='?',value=np.nan)# 丢弃有缺失值的样本data = data.dropna(how='any')print(data.shape) 1(683, 11) 123456789# 4. 解析分类数据# 由于每个维度的特征都是数字，所以我们就不用进行独热编码了# 5. 拆分数据集X_train, X_test, Y_train, Y_test = train_test_split( data.iloc[:,1:10] , data.iloc[:,10] , test_size = 0.25, random_state = 0)print(X_train.shape)print(Y_train.shape)print(X_test.shape)print(Y_test.shape) 1234(512, 9)(512,)(171, 9)(171,) 1234# 6. 特征缩放sc_X = StandardScaler()X_train = sc_X.fit_transform(X_train) # fit_transform 先训练数据得到方差期望等信息，再进行缩放X_test = sc_X.transform(X_test) # transform直接进行数据缩放，用的是X_train的方差和期望 1234567891011121314151617181920212223242526272829303132333435363738# 7. 逻辑回归# 在逻辑回归中使用梯度下降# 损失函数def loss_function(theta,x,y): y_hat = sigma(x.dot(theta)) return -np.sum(y*np.log(y_hat)+(1-y)*np.log(1-y_hat)) / len(x)# sigma函数# 所有运算都是矩阵运算def sigma(x): return 1/(1+np.exp(-x))# 梯度def tidu(theta,x,y): return x.T.dot(sigma(x.dot(theta))-y)/len(x)# 加一列截距X_train_b = np.insert(X_train,0,values=1,axis=1)X_test_b = np.insert(X_test,0,values=1,axis=1)Y_train_b = Y_train.values.reshape(Y_train.shape[0],1)print(Y_train_b.shape)alpha = 0.001theta_old = np.full((X_train_b.shape[1],1), -1)theta_new = np.ones((X_train_b.shape[1],1),dtype=int)dif=1e-8j=0# while abs(loss_function(theta_new,X_train_b,Y_train_b)-loss_function(theta_old,X_train_b,Y_train_b)) &gt; dif:for i in range(1441): theta_old = theta_new theta_new = theta_new-alpha*tidu(theta_new,X_train_b,Y_train_b) #print(theta_new) # print(loss_function(theta_new,X_train_b,Y_train_b)) j+=1print(j)print(theta_new) 123456789101112(512, 1)1441[[4.29250937] [1.48160367] [1.57861817] [1.57570445] [1.49454621] [1.5039255 ] [1.58484673] [1.53360512] [1.52989014] [1.29539431]] 其实这里应该再加一个评估算法，最起码应该有个混淆矩阵来展示训练的准确性，但是我懒得写了，因为后面会有专门的一节来写模型评估 随机梯度下降123# 随机梯度下降def tidu_r(theta,x,y): return x*(sigma(theta.T.dot(x))-y)/len(x) 12345678910111213import randomtheta_old = np.full((X_train_b.shape[1],1), -1)theta_new = np.ones((X_train_b.shape[1],1),dtype=int)while abs(loss_function(theta_new,X_train_b,Y_train_b)-loss_function(theta_old,X_train_b,Y_train_b)) &gt; dif:# for i in range(500): for j in range(X_train_b.shape[0]): alpha = 4/(1.0+i+j)+0.01 index = int(random.uniform(0, len(Y_train))) # print(index) # print(X_train_b[index][:, np.newaxis]) theta_old = theta_new theta_new = theta_new-alpha*tidu_r(theta_new,X_train_b[index][:, np.newaxis],Y_train_b[index])print(theta_new) 12345678910[[5.29632966] [1.54190479] [1.72308911] [1.76382384] [1.64690799] [1.75827617] [1.84435603] [1.55664591] [1.64353605] [1.36634795]] sklearn逻辑回归123from sklearn.linear_model import LogisticRegressionclassifier = LogisticRegression()classifier.fit(X_train, Y_train) 1234LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=&apos;ovr&apos;, n_jobs=1, penalty=&apos;l2&apos;, random_state=None, solver=&apos;liblinear&apos;, tol=0.0001, verbose=0, warm_start=False) 1y_pred = classifier.predict(X_test) 123# 混淆矩阵from sklearn.metrics import confusion_matrixcm = confusion_matrix(Y_test, y_pred) 1cm 12array([[103, 4], [ 5, 59]], dtype=int64) 1help(LogisticRegression) 12print(classifier.intercept_)print(classifier.coef_) 123[-1.14177747][[0.96836256 0.44066646 0.74714873 0.5553863 0.5467849 1.42317281 0.75210949 0.68330651 0.36139182]] sklearn 随机梯度下降12345# sklearn SDGfrom sklearn.linear_model import SGDClassifiersgdc=SGDClassifier()sgdc.fit(X_train,Y_train)sgdc_y_predict=sgdc.predict(X_test) 1help(SGDClassifier) ​ 12print(sgdc.coef_)print(sgdc.intercept_) 123[[ 3.00712617 6.44910932 1.9034264 7.9184444 -1.24606701 9.32825779 14.67255746 2.70862189 4.92001942]][-5.61627899] 123from sklearn.metrics import confusion_matrixcm = confusion_matrix(Y_test, sgdc_y_predict)cm 12array([[104, 3], [ 4, 60]], dtype=int64)","link":"/2019/05/26/ji-qi-xue-xi-bi-ji-ti-du-xia-jiang-yu-sui-ji-ti-du-xia-jiang/"},{"title":"机器学习笔记-----数据预处理","text":"数据预处理是机器学习中非常重要的步骤，为了能正确的分析数据，得到最佳的机器学习算法，拿到数据后我们一般来说都需要对数据进行预处理。数据预处理包括以下几个步骤： 导入需要的库 导入数据集 处理丢失数据 解析分类数据 拆分数据集分成训练集合和测试集合 特征缩放 图片来源与github项目机器学习100天中文翻译, 喜欢的可以去点个star哦 1. 导入库123import numpy as npimport pandas as pd 2. 导入数据集1234dataset = pd.read_csv(r'E:\\Downloads\\100-Days-Of-ML-Code-master\\100-Days-Of-ML-Code-master\\datasets\\Data.csv')X = dataset.iloc[ : , :-1].values # loc 表示标签索引行数据Y = dataset.iloc[ : , -1].values # iloc 表示按照行号号索引print(Y) 1[&apos;No&apos; &apos;Yes&apos; &apos;No&apos; &apos;No&apos; &apos;Yes&apos; &apos;Yes&apos; &apos;No&apos; &apos;Yes&apos; &apos;No&apos; &apos;Yes&apos;] 1dataset 1234567891011Country Age Salary PurchasedFrance 44 72000 NoSpain 27 48000 YesGermany 30 54000 NoSpain 38 61000 NoGermany 40 YesFrance 35 58000 YesSpain 52000 NoFrance 48 79000 YesGermany 50 83000 NoFrance 37 67000 Yes 3. 处理丢失数据123456from sklearn.preprocessing import Imputer# axis=0表示按列的平均值进行填充imputer = Imputer(missing_values = \"NaN\", strategy = \"mean\", axis = 0)imputer = imputer.fit(X[ : , 1:3]) # 处理缺失值X[ : , 1:3] = imputer.transform(X[ : , 1:3]) # 转换X 上述代码使用的是样本的平均值来代替缺失值, 也可以使用中位数、众数，接口详细信息参见help(Imputer) 4. 解析分类数据为什么要独热编码？ 使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，会让特征之间的距离计算更加合理。 为什么特征向量要映射到欧式空间？ 将离散特征通过one-hot编码映射到欧式空间，是因为，在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间 关于OneHotEncoder独热编码和 LabelEncoder标签编码 12345678910from sklearn.preprocessing import LabelEncoder, OneHotEncoderlabelencoder_X = LabelEncoder() # 对与不连续的变量进行编号X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])#Creating a dummy variableonehotencoder = OneHotEncoder(categorical_features = [0]) # 进行虚位掩码X = onehotencoder.fit_transform(X).toarray()labelencoder_Y = LabelEncoder()Y = labelencoder_Y.fit_transform(Y)print(Y)print(X[:,0]) 12[0 1 0 0 1 1 0 1 0 1][0. 1. 1. 1. 1. 0. 1. 0. 1. 0.] 5. 拆分数据集分成训练集合和测试集合123# 训练集和测试集from sklearn.model_selection import train_test_splitX_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0) 关于怎么拆分数据集 拆分数据的目的是对模型进行评估，如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分成三部分，分别为训练集（training set）、验证集（validation set）和测试集（test set）, 比例一般是 6:2:2 但是，在许多实际应用中数据是不充足的。为了选择好的模型，可以采用交叉验证方法。 交叉验证的基本想法是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。一般有 简单交叉验证， S折交叉验证， 留一交叉验证等方法 我们会在《模型评估与模型选择》一节在进行详细描述 6. 特征缩放12345# 标准化from sklearn.preprocessing import StandardScalersc_X = StandardScaler()X_train = sc_X.fit_transform(X_train) # fit_transform 先训练数据得到方差期望等信息，再进行缩放X_test = sc_X.transform(X_test) # transform直接进行数据缩放，用的是X_train的方差和期望 关于特征缩放 特征缩放就是将所有的特征分布转成成同一数量级，示例数据中的年龄与薪水的方差、均值等分布特征相差很大，这样会导致年龄对模型的影响会很小，其实就是均一化。 sklearn特征缩放StandardScaler 的计算公式: $$ x_{stand} = \\frac {x-mean(x)} {std} $$ 特征缩放的更多介绍:关于使用sklearn进行数据预处理 —— 归一化/标准化/正则化 参考资料 机器学习A-Z～数据预处理 机器学习100天","link":"/2019/05/17/ji-qi-xue-xi-bi-ji-shu-ju-yu-chu-li/"},{"title":"机器学习笔记-----线性回归","text":"线性模型的基本形式给定由d 个属性描述的示例$\\vec x = (x_1,x_2,…x_d)$ ， 其中$x_i$是x在第i个属性上的取值，线性模型(linear model)试图学得一个通过属性的线性组合来进行预测的函数，即公式（1）：$$ f(\\vec x) = \\omega_1 x_1 + \\omega_2 x_2 +\\cdots + \\omega_d x_d+d + b $$ 写成向量式，公式（2）：$$ f(\\vec x) = \\vec\\omega^T \\vec x $$ 这里的 $$\\vec\\omega^T = [\\omega_1,\\omega_2,\\cdots,\\omega_n]$$ $$\\vec x = \\left[ \\begin{matrix} x_1 \\\\ x_2 \\\\ \\cdots\\\\ x_{n-1}\\\\ 1 \\end{matrix} \\right]$$ 式(2)是单个样本的模型，如果是多个样本，那么: $$ X = \\left[ \\begin{matrix} x_1^{(1)} \\ x_2^{(1)} \\ x_1^{(1)} \\ \\cdots \\ x_{n-1}^{(1)} \\ 1 \\\\ x_1^{(2)} \\ x_2^{(2)} \\ x_3^{(2)} \\ \\cdots \\ x_{n-1}^{(2)} \\ 1 \\\\ x_1^{(3)} \\ x_2^{(3)} \\ x_3^{(3)} \\ \\cdots \\ x_{n-1}^{(3)} \\ 1 \\\\ \\cdots \\\\ x_1^{(m)} \\ x_2^{(m)} \\ x_3^{(m)} \\ \\cdots \\ x_{n-1}^{(m)} \\ 1 \\end{matrix} \\right] = \\left[ \\begin{matrix} \\vec x^{(1)T} \\ 1 \\\\ \\vec x^{(2)T} \\ 1 \\\\ \\vec x^{(3)T} \\ 1 \\\\ \\cdots \\\\ \\vec x^{(m)T} \\ 1 \\\\ \\end{matrix} \\right] $$ 因此多变量线性回归的公式就变成： $$f(x)= X^T \\vec\\omega $$ 线性回归原理给定数据集$D = {(\\vec x_1 ,y_1),(\\vec x_2,y_2),\\cdots,(\\vec x_m,y_m)}$,线性回归就是使用$\\vec x$得到一个线性模型是的输出$y’$ 无限接近与$y$，即： 公式（3）$$f(\\vec x ) = \\vec\\omega^T \\vec x \\simeq y $$这里忽略了下标，即每一个输入$\\vec x$ 通过线性回归模型的式子$(3)$得到的$y$ 都无限接近与数据集中的$y$ 代价函数那么如何确定公式$(3)$中的$\\vec\\omega$呢? 如果我们的$f(x)$ 与$y$的差别足够小，那么我们就可以带入到公式$(3)$中直接解出$\\vec\\omega$和$b$来: $$min\\sum|f(x)-y|$$ 而在实际运用过程中我们一般用平方误差来做性能度量，公式（4）： $$min \\sum(f(x)-y)^2 $$ 当然也有其他的性能度量方式，只是平方误差这个在线性回归中最常用，这个求解线性回归参数的方法也叫做最小二乘法令： 公式（5）$$J (\\omega) = \\sum(f(x)-y)^2 $$ $J(\\omega)$在这里就叫做代价函数,吴恩达老师的机器学习课程中稍做了调整： $$J(\\omega) = \\frac{1}{2m} \\sum_{i=1}^m (f(x)-y)^2 $$ 写成向量形式就是：$$J(\\mathbf\\omega) = \\frac{1}{2}(\\mathbf{X\\vec\\omega} -\\mathbf{Y})^T(\\mathbf{X\\vec\\omega} - \\mathbf{Y})$$ *至于为什么加了1/2是因为在损失函数求导的过程中可以消去2，而1/m为了减少数据量对梯度的影响* 我们只需要对$J(\\omega)$求偏导就能够求得最小值，这里假设$J(\\omega)$是凸函数 代价函数最小值求最小值可以使用梯度下降法，在这之前，我们先讲如何直接解出$J(\\omega)$的最小值： 求导法 矩阵法 显然这种方法比较复杂，而且逆矩阵也不一定存在，,貌似计算量也很大 下面讲另一种求解$J(\\omega)$最小值的方式， 梯度下降法该方法的核心思想是同步更新$\\vec\\omega$中每个值： for j in: len(w):$$\\omega_j = \\omega_j -\\alpha \\sum_{i=1}^m \\frac{\\partial J(\\omega) }{\\partial \\omega_j}$$ $\\alpha$ 称作学习率,它的值太小会增加求解的时间，太大会容易造成过拟合。该方法像是一种穷举，一步一步的找到最小值。关于这种方法的实现我们将会在逻辑回归中演示，下面讲一下Scikit-Learn关于线性回归的用法。 LinearRegression参考sklearn的官方文档中文版 linear_model.LinearRegression类的成员函数 fit 以数组X和y为输入，并将线性模型的系数$\\omega$存储在其成员变量coef_ 中： 123456&gt;&gt;&gt; from sklearn import linear_model&gt;&gt;&gt; reg = linear_model.LinearRegression()&gt;&gt;&gt; reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)&gt;&gt;&gt; reg.coef_array([ 0.5, 0.5]) 然而，最小二乘的系数估计依赖于模型特征项的独立性。当特征项相关并且设计矩阵X 的列近似的线性相关时，设计矩阵便接近于一个奇异矩阵(不是满秩，列数过多，会有多个最优解，需要正则化)，因此最小二乘估计对观测点中的随机误差变得高度敏感，产生大的方差。 手动实现线性回归这里我们使用矩阵方法求参数 1234567891011121314151617181920212223242526272829303132333435363738import numpy as npimport pandas as pdimport matplotlibimport matplotlib.pyplot as pltfrom sklearn import datasetsboston = datasets.load_boston()X = boston.dataY = boston.targetX = X[Y &lt; 50.0]Y = Y[Y &lt; 50.0]from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)# print(X.shape)# print(Y.shape)# (490, 13)# (490,)# 加一列截距X_train_b = np.insert(X_train,0,values=1,axis=1)X_test_b = np.insert(X_test,0,values=1,axis=1)# print(y_train.shape)y_train_b = y_train.reshape(y_train.shape[0],1)theta = np.linalg.inv(X_train_b.T.dot(X_train_b)).dot(X_train_b.T).dot(y_train_b)print(theta)# 使用sklearnfrom sklearn import linear_modelreg = linear_model.LinearRegression()reg.fit(X,Y)print(reg.intercept_)print(reg.coef_)# print(dir(reg))","link":"/2019/05/17/ji-qi-xue-xi-bi-ji-xian-xing-hui-gui/"},{"title":"python3 标准库系列(四)---logging","text":"logging模块可以给脚本添加日志功能。 下面是一个输出日志的简单版： 1234567891011121314151617import logging# levelname 输出等级# asctime 时间# lineno 所在行# module 所在模块名# filename 日志输出函数的模块的文件名，注意不是参数 filename# message 输出的内容logging.basicConfig(format='%(levelname)s %(asctime)s,line %(lineno)s:\\t%(message)s', level=logging.INFO, datefmt='%Y-%m-%d %H:%M:%S') # 可以加filename=logfile参数将日志输出到文件中logging.critical('当发生严重错误，导致应用程序不能继续运行时记录的信息')logging.error('由于一个更严重的问题导致某些功能不能正常运行时记录的信息')logging.warning('当某些不期望的事情发生时记录的信息（如，磁盘可用空间较低），但是此时应用程序还是正常运行的')logging.info('信息详细程度仅次于DEBUG，通常只记录关键节点信息，用于确认一切都是按照我们预期的那样进行工作')logging.debug('最详细的日志信息，典型应用场景是 问题诊断') 日志调用（critical(), error(), warning(), info(), debug()）以降序方式表示不同的严重级别,默认是warning。 下面的是一个自己用的以天为单位的输出日志的函数： 123456789101112131415161718192021222324252627import reimport loggingfrom logging.handlers import TimedRotatingFileHandlerfrom logging.handlers import RotatingFileHandlerdef log_init(logfile): log_fmt = '%(asctime)s\\tFile \\\"%(filename)s\\\",line %(lineno)s\\t%(levelname)s: %(message)s' formatter = logging.Formatter(log_fmt) # 可以设置间隔时间，这里是一天生成一个文件 log_file_handler = TimedRotatingFileHandler(filename=logfile, when=\"D\", interval=1, backupCount=7) log_file_handler.suffix = \"%Y-%m-%d.log\" # filehanlder.suffix的格式必须这么写，才能自动删除旧文件 # 如果设定是天，就必须写成“%Y-%m-%d.log”，写成其他格式会导致删除旧文件不生效 log_file_handler.setFormatter(formatter) # log_file_handler.setLevel(logging.DEBUG) logging.getLogger().setLevel(logging.DEBUG) logging.getLogger().addHandler(log_file_handler) ''' 这里是需要运行的程序，需要按照文件格式删除log文件，需要启用 extMatch、removeHandle函数 ''' # logging.info('this is a loggging info message') # logging.debug('this is a loggging debug message') # logging.warning('this is loggging a warning message') # logging.error('this is an loggging error message') # logging.critical('this is a loggging critical message') 修改所调用的模块中的日志设置： 123456789# somelib.pyimport logginglog = logging.getLogger(__name__) # 默认就是： __name__ 只的当前的模块log.addHandler(logging.NullHandler())# Example function (for testing)def func(): log.critical('A Critical Error!') log.debug('A debug message') 下面我们调用这个模块： 123456789101112&gt;&gt;&gt; import logging# 这是当前脚本的日志设置&gt;&gt;&gt; logging.basicConfig(level=logging.ERROR)&gt;&gt;&gt; import somelib&gt;&gt;&gt; somelib.func()CRITICAL:somelib:A Critical Error!&gt;&gt;&gt; # 指定模块名，改变该模块的输出级别&gt;&gt;&gt; logging.getLogger('somelib').level=logging.DEBUG&gt;&gt;&gt; somelib.func()CRITICAL:somelib:A Critical Error!DEBUG:somelib:A debug message&gt;&gt;&gt; 在这里，根日志被配置成仅仅输出ERROR 或更高级别的消息。不过，somelib 的日志级别被单独配置成可以输出debug 级别的消息，它的优先级比全局配置高。像这样更改单独模块的日志配置对于调试来讲是很方便的，因为你无需去更改任何的全局日志配置——只需要修改你想要更多输出的模块的日志等级。","link":"/2018/11/08/python3-biao-zhun-ku-xi-lie-si-logging/"},{"title":"机器学习笔记-----Logistic回归","text":"Logistic回归,有翻译为“逻辑回归”，但是它跟“逻辑”这个单词一点关系都没有，也不是解决回归问题的，是用来解决二分类的算法，最准确的翻译应该是“对数几率回归” 至于为什么叫对数几率刚回归，请看下面的推倒过程 线性回归回顾先来看一个简单的线性回归模型： $$ f(x) = \\vec\\omega^T \\vec{x}+b $$这里的 $$ \\vec\\omega^T = [\\omega_1,\\omega_2,\\cdots,\\omega_n] $$ $$ \\vec x = \\left[ \\begin{matrix} x_1 \\\\ x_2 \\\\ \\cdots\\\\ x_n \\end{matrix} \\right] $$ 一般来说给定的数据集是：$D={(\\vec x^{(1)},y^{(1)}),(\\vec x^{(2)},y^{(2)}),⋯,(\\vec x^{(m)},y^{(m)})}$, 即有m组数据，n个未知数，因此多变量线性回归的公式是： $$f(x)= X^T \\vec\\omega $$ 其中： $$X = \\left[ \\begin{matrix} x_1^{(1)} \\ x_2^{(1)} \\ x_1^{(1)} \\ \\cdots \\ x_n^{(1)} \\ 1 \\\\ x_1^{(2)} \\ x_2^{(2)} \\ x_3^{(2)} \\ \\cdots \\ x_n^{(2)} \\ 1 \\\\ x_1^{(3)} \\ x_2^{(3)} \\ x_3^{(3)} \\ \\cdots \\ x_n^{(3)} \\ 1 \\\\ \\cdots \\\\ x_1^{(m)} \\ x_2^{(m)} \\ x_3^{(m)} \\ \\cdots \\ x_n^{(m)} \\ 1 \\end{matrix} \\right] = \\left[ \\begin{matrix} \\vec x^{(1)T} \\ 1 \\\\ \\vec x^{(2)T} \\ 1 \\\\ \\vec x^{(3)T} \\ 1 \\\\ \\cdots \\\\ \\vec x^{(m)T} \\ 1 \\\\ \\end{matrix} \\right]$$ 展开式： $$\\omega_1 x_1^{(1)} + \\omega_2 x_2^{(1)} + \\cdots + \\omega_n x_n^{(1)} +b = y^{(1)} $$ $$\\omega_1 x_1^{(2)} + \\omega_2 x_2^{(2)} + \\cdots + \\omega_n x_n^{(2)} + b = y^{(2)}$$ $$\\cdots$$ $$\\omega_1 x_1^{(m)} + \\omega_2 x_2^{(m)} + \\cdots + \\omega_n x_n^{(m)} +b = y^{(m)}$$ 我们为了便于推导，忽略了下标，只用了一组$(\\vec{x},y)$,然后把$b$当做$\\omega_0$，而此时$x_0 = 1$： $$f(x)=\\vec\\omega^T \\vec{x} $$ sigmoid函数线性回归很显然是一个连续的值，既然要解决二分类的问题，那最好是换成一个$0/1$的值 这里引出一个sigmoid函数：$$g(z) = \\frac {1}{1+e^{-z}}$$ 令： $$z =\\vec\\omega^T \\vec{x}+b $$ $$f(x) = g(z) = \\frac {1}{1+e^{-\\vec\\omega^T \\vec{x}}} $$ 我们就引出了我们的模型： $$ln \\frac{y}{1-y} = \\vec\\omega^T \\vec{x}+b $$ 上式中$\\frac{y}{1-y}$ 称为几率，反应x获得正例(y)的相对可能性，由此可以看出，模型实际上是在用线性回归模型的预测结果去逼近真实值$y$的对数几率，因此该模型就称为对数几率回归模型 代价函数由于$y$的值为0,1，我们将$y$视为后验概率： $$p(y=1 | x;\\omega) = f(x) $$ $$p(y=0 | x;\\omega) = 1- f(x) $$ $f(x)$越大，代表着预测值与$y$越接近，误差就越小，带入到公式中： $$p(y=1 | x;\\omega) = \\frac {e^{\\vec\\omega^T \\vec{x}}}{1+e^{\\vec\\omega^T \\vec{x}}} $$ $$p(y=0 | x;\\omega) = \\frac {1}{1+e^{\\vec\\omega^T \\vec{x}}} $$ 为了确定$\\omega$用对数似然函数： $$ \\ell(\\omega) = lnp(y|x ;\\omega)$$ 可以合并到一起： $$ p(y|x ;\\omega) = f(x)^y(1-f(x))^{(1-y)}$$ 带入到对数似然函数中： $$ln p(y|x ;\\omega) = y lnf(x) + (1-y)ln(1-f(x)) $$ 我们得到的代价函数： $$J(\\omega) = - \\frac{1}{m} \\sum_{i=1}^m [ y^{(i)} lnf(x^{(i)}) + (1-y^{(i)})ln(1-f(x^{(i)})) ] $$ $J(\\omega)$越小，就说明$f(x) = 1$的概率越大，因此只要求出$J(\\omega)$的最小值 ps：1） 之前推倒时候忽略了上标，但是我们在最后的损失函数中为了完整性就把上标加了上去 梯度下降法对代价函数求导的过程就不写了，直接写最后的结果: $$\\frac{\\partial J(\\omega)}{\\partial\\omega_j} =\\frac{1}{m} \\sum_{i=1}^m (f(x^{(i)}) - y^{(i)})x_j^{(i)} $$ 或者是： $$\\frac{\\partial}{\\partial\\mathbf\\omega}J(\\mathbf\\omega) = \\frac{1}{m}\\mathbf{X}^T(\\mathbf{X\\vec\\omega} - \\mathbf{Y})$$ $\\omega$的更新公式： $$\\omega_{j} := \\omega_j- \\frac{1}{m}\\alpha\\sum_{i=1}^m(f(x^{(i)}) - y^{(i)})x_j^{(i)} $$ 其中, $j=1,2,…n$，需要注意的是$n$个$\\omega$是同时更新的 写成向量形式就是： $$\\omega_{j} := \\omega_j- \\frac{1}{m} \\alpha \\mathbf {X}^T (g(\\mathbf{X} \\vec\\omega) - \\vec y) $$ sklearn代码实现逻辑回归1234%matplotlib inlineimport numpy as npimport matplotlib.pyplot as pltimport pandas as pd 12dataset = pd.read_csv(r'E:\\Downloads\\100-Days-Of-ML-Code-master\\Social_Network_Ads.csv')dataset.head(5) 12X = dataset.iloc[:,[2,3]].valuesY = dataset.iloc[:,4].values 12from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0) 1234from sklearn.preprocessing import StandardScalersc = StandardScaler()X_train = sc.fit_transform(X_train)X_test = sc.transform(X_test) 1234from sklearn.linear_model import LogisticRegressionclassifier = LogisticRegression()classifier.fit(X_train, y_train)# classifier.coef_ 1y_pred = classifier.predict(X_test) 12from sklearn.metrics import confusion_matrixcm = confusion_matrix(y_test, y_pred) 可视化 1234567891011121314151617181920212223242526272829303132333435from matplotlib.colors import ListedColormapX_set,y_set=X_train,y_trainX1,X2=np. meshgrid(np. arange(start=X_set[:,0].min()-1, stop=X_set[:, 0].max()+1, step=0.01), np. arange(start=X_set[:,1].min()-1, stop=X_set[:,1].max()+1, step=0.01))plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))plt.xlim(X1.min(),X1.max())plt.ylim(X2.min(),X2.max())for i,j in enumerate(np. unique(y_set)): plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1], c = ListedColormap(('red', 'green'))(i), label=j)plt. title(' LOGISTIC(Training set)')plt. xlabel(' Age')plt. ylabel(' Estimated Salary')plt. legend()plt. show()X_set,y_set=X_test,y_testX1,X2=np. meshgrid(np. arange(start=X_set[:,0].min()-1, stop=X_set[:, 0].max()+1, step=0.01), np. arange(start=X_set[:,1].min()-1, stop=X_set[:,1].max()+1, step=0.01))plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))plt.xlim(X1.min(),X1.max())plt.ylim(X2.min(),X2.max())for i,j in enumerate(np. unique(y_set)): plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1], c = ListedColormap(('red', 'green'))(i), label=j)plt. title(' LOGISTIC(Test set)')plt. xlabel(' Age')plt. ylabel(' Estimated Salary')plt. legend()plt. show()","link":"/2019/01/23/ji-qi-xue-xi-bi-ji-logistic-hui-gui/"},{"title":"生新基础知识-----sam-bam文件格式详解","text":"SAM是一种序列比对格式标准，由sanger制定，是以TAB为分割符的文本格式。 主要应用于测序序列mapping到基因组上的结果表示，当然也可以表示任意的多重比对结果。 SAM的全称是sequence alignment/map format 定义和示例SAM分为两部分，注释信息（header section ）和比对结果部分 （alignment section）。 通常是把FASTQ文件格式的测序数据比对到对应的参考基因组版本得到的。 注释信息并不是SAM文件的重点，是该SAM文件产生以及被处理过程的一个记录，规定以@开头，用不同的tag表示不同的信息，主要有： @HD，说明符合标准的版本、对比序列的排列顺序； @SQ，参考序列说明； @RG，比对上的序列（read）说明； @PG，使用的程序说明； @CO，任意的说明信息。一个简单的SAM文件例子如下： 1ST-E00317:118:HNHK2CCXX:7:1101:5071:1309 97 chr1 196695666 60 147M chr18 54806715 0 AAGAATATGGACACAGTGAAGTGGTGGAATATTATTGCAATCCTAGATTTCTAATGAAGGGACCTAATAAAATTCAGTGTGTTGATGGAGAGTGGACAACTTTACCAGTGTGTATTTGTAATGTATAAAACATTAATATTGAAACTT FFAKKKKKKFKKKKKK,FKKKKKKK&lt;KKKFKK,FKKKKKKKKKFFKKKKKFFKKFAKFK,FKKKAKKKKKKKKKKKAF,,,FFAFA,FKKAKAA7FKKFKFFFKKKK7FFKK7F7K,7FAFKAFFKKKKK,7AFKFKFA,,,AAK,A MC:Z:13S134M MD:Z:76A39G13T9T6 RG:Z:sampleID NM:i:4 AS:i:127 XS:i:21 说明如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354QNAME ST-E00317:118:HNHK2CCXX:7:1101:5071:1309 ## read名称FLAG 97 ## FLAG, 概括出一个合适的标记，各个数字分别代表##1 序列是一对序列中的一个##2 比对结果是一个pair-end比对的末端##4 没有找到位点##8 这个序列是pair中的一个但是没有找到位点##16 在这个比对上的位点，序列与参考序列反向互补##32 这个序列在pair-end中的的mate序列与参考序列反响互补##64 序列是 mate 1##128 序列是 mate 2##假如说标记为以上列举出的数目，就可以直接推断出匹配的情况。假如说标记不是以上列举出的数字，比如说83=（64+16+2+1），就是这几种情况值和。## 其他情况可参见 http://broadinstitute.github.io/picard/explain-flags.htmlRNAME chr1 ## 染色体名称POS 196695666 ## 染色体位置MAQ 60 ## mapping质量 越高说明位点越独特CIGAR 147M ## “M”表示 match或 mismatch；## “I”表示 insert；## “D”表示 deletion；## “N”表示 skipped（跳过这段区域）；## “S”表示 soft clipping（被剪切的序列存在于序列中）；## “H”表示 hard clipping（被剪切的序列不存在于序列中）；## “P”表示 padding；## “=”表示 match；## “X”表示 mismatch（错配，位置是一一对应的）；MRNM chr18## read2 在参考序列上的位置，如果没有就用 &quot;*&quot;, 相同&quot;=&quot;MPOS 0 ##read2 的位置，若不可用则用0ISIZE 0## 插入片段的长度，最左边得为正，最右边的为负，中间的不用定义正负，不分区段（single-segment)的比对上，或者不可用时，此处为0SEQ AAGAATATGGACACAGTGAAGTGGTGGAATATTATTGCAATCCTAGATTTCTAATGAAGGGACCTAATAAAATTCAGTGTGTTGATGGAGAGTGGACAACTTTACCAGTGTGTATTTGTAATGTATAAAACATTAATATTGAAACTT## 序列片段的序列信息QUAL FFAKKKKKKFKKKKKK,FKKKKKKK&lt;KKKFKK,FKKKKKKKKKFFKKKKKFFKKFAKFK,FKKKAKKKKKKKKKKKAF,,,FFAFA,FKKAKAA7FKKFKFFFKKKK7FFKK7F7K,7FAFKAFFKKKKK,7AFKFKFA,,,AAK,A## 序列的质量信息，格式同FASTQ一样。read质量的ASCII编码MC:Z:13S134M MD:Z:76A39G13T9T6 RG:Z:sampleID NM:i:4 AS:i:127 XS:i:21## 可选字段（optional fields)，格式如：TAG:TYPE:VALUE，其中TAG有两个大写字母组成，每个TAG代表一类信息，每一行一个TAG只能出现一次，TYPE表示TAG对应值的类型，可以是字符串、整数、字节、数组等。## AS:i 最佳匹配的得分## XS:i 第二好的匹配的得分## YS:i mate 序列匹配的得分## XN:i 在参考序列上模糊碱基的个数## XM:i 错配的个数## XO:i gap open的个数## XG:i gap 延伸的个数## NM:i 经过编辑的序列 ## YF:i 说明为什么这个序列被过滤的字符串## YT:Z## MD:Z 代表序列和参考序列错配的字符串## 参考链接： SAM(file format)(可下载PDF文件) https://genome.sph.umich.edu/wiki/SAM#What_are_TAGs.3F https://mp.weixin.qq.com/s/yK1OyJHrePg6bWl41JCpvA http://www.cnblogs.com/emanlee/p/5366610.html","link":"/2019/05/18/sheng-xin-ji-chu-zhi-shi-sam-bam-wen-jian-ge-shi/"}],"tags":[{"name":"摘抄","slug":"摘抄","link":"/tags/摘抄/"},{"name":"awk","slug":"awk","link":"/tags/awk/"},{"name":"python3","slug":"python3","link":"/tags/python3/"},{"name":"机器学习","slug":"机器学习","link":"/tags/机器学习/"},{"name":"生信基础","slug":"生信基础","link":"/tags/生信基础/"},{"name":"python","slug":"python","link":"/tags/python/"}],"categories":[{"name":"阅读","slug":"阅读","link":"/categories/阅读/"},{"name":"计算机","slug":"计算机","link":"/categories/计算机/"},{"name":"数学与统计","slug":"数学与统计","link":"/categories/数学与统计/"},{"name":"生信","slug":"生信","link":"/categories/生信/"}]}