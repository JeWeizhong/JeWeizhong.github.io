{"pages":[{"title":"about","text":"","link":"/about/index.html"},{"title":"archives","text":"","link":"/archives/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"friends","text":"","link":"/friends/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"AWK简明教程","text":"AWK是一种处理文本文件的语言，是一个强大的文本分析工具。之所以叫AWK是因为其取了三位创始人 Alfred Aho，Peter Weinberger, 和 Brian Kernighan 的 Family Name 的首字符。 awk命令格式和选项语法形式 12awk [options] &apos;script&apos; var=value file(s)awk [options] -f scriptfile var=value file(s) 常用命令选项 -F fs fs指定输入分隔符，fs可以是字符串或正则表达式，如-F: -v var=value 赋值一个用户定义变量，将外部变量传递给awk -f scripfile 从脚本文件中读取awk命令 -m[fr] val 对val值设置内在限制，-mf选项限制分配给val的最大块数目；-mr选项限制记录的最大数目。这两个功能是Bell实验室版awk的扩展功能，在标准awk中不适用。 工作原理 1hawk &apos;BEGIN{ commands } pattern{ commands } END{ commands }&apos; 内建变量 变量 详细说明 $0 当前记录（这个变量中存放着整个行的内容） $1~$n 当前记录的第n个字段，字段间由FS分隔 FS 输入字段分隔符 默认是空格或Tab (也可以是-F) NF 当前记录中的字段个数，就是有多少列 NR 已经读出的记录数，就是行号，从1开始，如果有多个文件话，这个值也是不断累加中。 FNR 当前记录数，与NR不同的是，这个值会是各个文件自己的行号 RS 输入的记录分隔符， 默认为换行符 OFS 输出字段分隔符， 默认也是空格 ORS 输出的记录分隔符，默认为换行符 FILENAME 当前输入文件的名字 ##### 模式匹配 1$ awk '$6 ~ /FIN|TIME/ || NR==1 {print NR,$4,$5,$6}' OFS=\"\\t\" netstat.txt 统计下面的命令计算所有的C文件，CPP文件和H文件的文件大小总和。 12$ ls -l *.cpp *.c *.h | awk &apos;{sum+=$5} END {print sum}&apos;2511401 统计每个用户的进程的占了多少内存 123456$ ps aux | awk 'NR!=1{a[$1]+=$6;} END { for(i in a) print i \", \" a[i]\"KB\";}'dbus, 540KBmysql, 99928KBwww, 3264924KBroot, 63644KBhchen, 6020KB 控制结构和脚本语言待补充 参考链接： http://man.linuxde.net/awkhttps://coolshell.cn/articles/9070.html","link":"/2018/02/16/awk-jian-ming-jiao-cheng/"},{"title":"沈向洋：You Are What You Write","text":"You Are What You Write原文：https://www.linkedin.com/pulse/you-what-write-harry-shum?from=timeline&amp;isappinstalled=0 Are Twitter, PowerPoint, Facebook, Instagram and texting eroding our ability to think? There is a Chinese proverb that says “见文如见人,” which literally means “reading the document is the same as seeing the author.” If we are what we write, then who have we, as a society, become? I was sitting in a technical review recently, listening to one of our reviewers grill the engineer who was presenting: Why did you choose that design? Why is the service showing bad results? How many users will switch to the solution? The presenter’s answers lacked depth. It seemed like he hadn’t done enough rigorous thinking, the kind where you sit quietly, sift through research, contemplate options, determine what you know, don’t know and where more work is required. The kind of thinking I did as a young researcher when peers took me and my work apart when I took short cuts. Back then, I practiced a disciplined approach, spending hours just thinking, and even more hours on the hardest part—writing it down. Today, long-form writing is being replaced. Tweets pass for dialogue. PowerPoint condenses thoughts to bullets. Words have been traded for emojis and GIFs. And we’ve become addicted to the noise. What happens in an Internet minute? 16 million text messages. 1.8 million snaps. 452,000 Tweets. 156 million emails. Who has time to think, let alone write? And maybe we, in the technology industry, have shaped this reality. We created the phones, apps and 24/7-connected world. We’ve enabled society to put down the pen. The only writing I do today is email or quick WeChat posts. So now I worry that we’re losing a valuable tool that helps us to think deeply, express who we are at our greatest and expand the intellect of those around us. And for us in the technical community, this is especially troubling. The stakes are higher than ever before with AI. We’re under enormous pressure to ship quickly, to achieve more, faster, but we can’t do this at the expense of the highest engineering quality. We have to think carefully about consequences and alternatives. Who gets blamed when a self-driving car hits someone? The engineer who wrote the code is the driver. Who is accountable for the AI algorithm with bias? The engineer who created the AI. I see fewer engineers writing and sharing deep thinking, but this is what will lead to far more true innovation across the industry. How will we achieve the big transformative breakthroughs versus the incremental milestones? By writing. Because the way to think is actually to write. Putting pen to paper forces you to develop and refine your thinking by iterating, revising and exploring alternatives. Anyone who can think deeply can write beautiful code, inspiring papers or develop the plan to bring the next big thing to life. I encourage you to read Reid Hoffman’s Series B pitch for LinkedIn in which he shares the thinking that helped him succeed. At the time, he shares that a partner in a venture firm was exposed to around 5,000 pitches, looked more closely at 600 to 800, and did between 0 and 2 deals. Writing offers the possibility to create lasting artifacts. I think of papers I published that endure, albeit perhaps as reference materials. Plenoptic Sampling. Lazy Snapping. Poisson Matting. These are my work’s contribution to the field of computer vision and graphics. They will survive me and, if I’m lucky, even help shape a mind or two. One of my favorite professors at Carnegie Mellon, Takeo Kanade, said that you have to write research papers like detective novels. You need story, suspense, surprise and ‘aha’ to explain your ideas to peers, to inspire others to contribute and advance your work and the whole field. Writing is an equalizer to get the best from the whole team. At Amazon, presentations are done with the six-page paper. Meetings kick off with everyone reading followed by comments and questions to the author. Everyone operates from the same context, and introverts, extroverts and non-native speakers have an equal chance to get their thinking across. It’s not about the presenter’s personality, but the words. Ultimately, writing helps make you successful. You might be the smartest person with the best idea, but if you can’t communicate your thinking in a compelling way, you won’t get far. Two engineers in our AI+R team who inspire me with their regular writing habits are Bill Ramsey and Ronny Kohavi. Bill has written over 250 blog posts at Microsoft, benefitting our entire technical community. With Ronny, you don’t even need to meet him—his highly cited A/B test experimentation papers say it all, and he’s publishing for the benefit of the industry on LinkedIn. As you’re reading this, you may be logging your objections: I need to drive results, so I need to go straight to code. I’m known for my code, so I don’t need to write papers. I’m not a native speaker, and I speak better with my code. I don’t know what to write about. I don’t have time… But please set them aside—for your own success, for your company’s, for the industry’s advancement—and start writing. I see so many occasions for building long-form writing back into the engineering culture—planning documents, project proposals, technology LRP’s, review articles—to inspire us to work together, collectively creating and cultivating big ideas and big thinking. I took a first step recently, writing a research paper with my colleagues Xiaodong He and Di Li, From Eliza to XiaoIce: Challenges and Opportunities with Social Chatbots, for the first time in years, so please no judgment, only constructive feedback! I challenge everyone reading this piece to write 500 words per week. If you’ve got an idea or you see a problem, write your proposal and share it! Let’s rewrite our standards for thought leadership and engineering quality by writing more!","link":"/2018/03/03/you-are-what-you-write/"},{"title":"机器学习笔记-----Logistic回归","text":"Logistic回归,有翻译为“逻辑回归”，但是它跟“逻辑”这个单词一点关系都没有，也不是解决回归问题的，是用来解决二分类的算法，最准确的翻译应该是“对数几率回归” 至于为什么叫对数几率刚回归，请看下面的推倒过程 线性回归回顾先来看一个简单的线性回归模型： $$ f(x) = \\vec\\omega^T \\vec{x}+b $$这里的 $$ \\vec\\omega^T = [\\omega_1,\\omega_2,\\cdots,\\omega_n] $$ $$ \\vec x = \\left[ \\begin{matrix} x_1 \\\\ x_2 \\\\ \\cdots\\\\ x_n \\end{matrix} \\right] $$ 一般来说给定的数据集是：$D={(\\vec x^{(1)},y^{(1)}),(\\vec x^{(2)},y^{(2)}),⋯,(\\vec x^{(m)},y^{(m)})}$, 即有m组数据，n个未知数，因此多变量线性回归的公式是： $$f(x)= X^T \\vec\\omega $$ 其中： $$X = \\left[ \\begin{matrix} x_1^{(1)} \\ x_2^{(1)} \\ x_1^{(1)} \\ \\cdots \\ x_n^{(1)} \\ 1 \\\\ x_1^{(2)} \\ x_2^{(2)} \\ x_3^{(2)} \\ \\cdots \\ x_n^{(2)} \\ 1 \\\\ x_1^{(3)} \\ x_2^{(3)} \\ x_3^{(3)} \\ \\cdots \\ x_n^{(3)} \\ 1 \\\\ \\cdots \\\\ x_1^{(m)} \\ x_2^{(m)} \\ x_3^{(m)} \\ \\cdots \\ x_n^{(m)} \\ 1 \\end{matrix} \\right] = \\left[ \\begin{matrix} \\vec x^{(1)T} \\ 1 \\\\ \\vec x^{(2)T} \\ 1 \\\\ \\vec x^{(3)T} \\ 1 \\\\ \\cdots \\\\ \\vec x^{(m)T} \\ 1 \\\\ \\end{matrix} \\right]$$ 展开式： $$\\omega_1 x_1^{(1)} + \\omega_2 x_2^{(1)} + \\cdots + \\omega_n x_n^{(1)} +b = y^{(1)} $$ $$\\omega_1 x_1^{(2)} + \\omega_2 x_2^{(2)} + \\cdots + \\omega_n x_n^{(2)} + b = y^{(2)}$$ $$\\cdots$$ $$\\omega_1 x_1^{(m)} + \\omega_2 x_2^{(m)} + \\cdots + \\omega_n x_n^{(m)} +b = y^{(m)}$$ 我们为了便于推导，忽略了下标，只用了一组$(\\vec{x},y)$,然后把$b$当做$\\omega_0$，而此时$x_0 = 1$： $$f(x)=\\vec\\omega^T \\vec{x} $$ sigmoid函数线性回归很显然是一个连续的值，既然要解决二分类的问题，那最好是换成一个$0/1$的值 这里引出一个sigmoid函数：$$g(z) = \\frac {1}{1+e^{-z}}$$ 令： $$z =\\vec\\omega^T \\vec{x}+b $$ $$f(x) = g(z) = \\frac {1}{1+e^{-\\vec\\omega^T \\vec{x}}} $$ 我们就引出了我们的模型： $$ln \\frac{y}{1-y} = \\vec\\omega^T \\vec{x}+b $$ 上式中$\\frac{y}{1-y}$ 称为几率，反应x获得正例(y)的相对可能性，由此可以看出，模型实际上是在用线性回归模型的预测结果去逼近真实值$y$的对数几率，因此该模型就称为对数几率回归模型 代价函数由于$y$的值为0,1，我们将$y$视为后验概率： $$p(y=1 | x;\\omega) = f(x) $$ $$p(y=0 | x;\\omega) = 1- f(x) $$ $f(x)$越大，代表着预测值与$y$越接近，误差就越小，带入到公式中： $$p(y=1 | x;\\omega) = \\frac {e^{\\vec\\omega^T \\vec{x}}}{1+e^{\\vec\\omega^T \\vec{x}}} $$ $$p(y=0 | x;\\omega) = \\frac {1}{1+e^{\\vec\\omega^T \\vec{x}}} $$ 为了确定$\\omega$用对数似然函数： $$ \\ell(\\omega) = lnp(y|x ;\\omega)$$ 可以合并到一起： $$ p(y|x ;\\omega) = f(x)^y(1-f(x))^{(1-y)}$$ 带入到对数似然函数中： $$ln p(y|x ;\\omega) = y lnf(x) + (1-y)ln(1-f(x)) $$ 我们得到的代价函数： $$J(\\omega) = - \\frac{1}{m} \\sum_{i=1}^m [ y^{(i)} lnf(x^{(i)}) + (1-y^{(i)})ln(1-f(x^{(i)})) ] $$ $J(\\omega)$越小，就说明$f(x) = 1$的概率越大，因此只要求出$J(\\omega)$的最小值 ps： 1）其实正常推倒出来的J(w)是没有前面的负号的，这里为了习惯，用梯度下降法求最小值，所以加了一个负号 2） 之前推倒时候忽略了上标，但是我们在最后的损失函数中为了完整性就把上标加了上去 梯度下降法对代价函数求导的过程就不写了，直接写最后的结果: $$\\frac{\\partial J(\\omega)}{\\partial\\omega_j} =\\frac{1}{m} \\sum_{i=1}^m (f(x^{(i)}) - y^{(i)})x_j^{(i)} $$ $$\\omega_{j+1} := \\omega_j- \\frac{1}{m}\\alpha\\sum_{i=1}^m(f(x^{(i)}) - y^{(i)})x_j^{(i)} $$ sklearn代码实现逻辑回归1234%matplotlib inlineimport numpy as npimport matplotlib.pyplot as pltimport pandas as pd 12dataset = pd.read_csv(r'E:\\Downloads\\100-Days-Of-ML-Code-master\\Social_Network_Ads.csv')dataset.head(5) 12X = dataset.iloc[:,[2,3]].valuesY = dataset.iloc[:,4].values 12from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0) 1234from sklearn.preprocessing import StandardScalersc = StandardScaler()X_train = sc.fit_transform(X_train)X_test = sc.transform(X_test) 1234from sklearn.linear_model import LogisticRegressionclassifier = LogisticRegression()classifier.fit(X_train, y_train)# classifier.coef_ 1y_pred = classifier.predict(X_test) 12from sklearn.metrics import confusion_matrixcm = confusion_matrix(y_test, y_pred) 可视化 1234567891011121314151617181920212223242526272829303132333435from matplotlib.colors import ListedColormapX_set,y_set=X_train,y_trainX1,X2=np. meshgrid(np. arange(start=X_set[:,0].min()-1, stop=X_set[:, 0].max()+1, step=0.01), np. arange(start=X_set[:,1].min()-1, stop=X_set[:,1].max()+1, step=0.01))plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))plt.xlim(X1.min(),X1.max())plt.ylim(X2.min(),X2.max())for i,j in enumerate(np. unique(y_set)): plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1], c = ListedColormap(('red', 'green'))(i), label=j)plt. title(' LOGISTIC(Training set)')plt. xlabel(' Age')plt. ylabel(' Estimated Salary')plt. legend()plt. show()X_set,y_set=X_test,y_testX1,X2=np. meshgrid(np. arange(start=X_set[:,0].min()-1, stop=X_set[:, 0].max()+1, step=0.01), np. arange(start=X_set[:,1].min()-1, stop=X_set[:,1].max()+1, step=0.01))plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))plt.xlim(X1.min(),X1.max())plt.ylim(X2.min(),X2.max())for i,j in enumerate(np. unique(y_set)): plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1], c = ListedColormap(('red', 'green'))(i), label=j)plt. title(' LOGISTIC(Test set)')plt. xlabel(' Age')plt. ylabel(' Estimated Salary')plt. legend()plt. show()","link":"/2019/01/23/ji-qi-xue-xi-bi-ji-logistic-hui-gui/"}],"tags":[{"name":"awk","slug":"awk","link":"/tags/awk/"},{"name":"摘抄","slug":"摘抄","link":"/tags/摘抄/"},{"name":"机器学习","slug":"机器学习","link":"/tags/机器学习/"}],"categories":[{"name":"计算机","slug":"计算机","link":"/categories/计算机/"},{"name":"阅读","slug":"阅读","link":"/categories/阅读/"},{"name":"数学与统计","slug":"数学与统计","link":"/categories/数学与统计/"}]}