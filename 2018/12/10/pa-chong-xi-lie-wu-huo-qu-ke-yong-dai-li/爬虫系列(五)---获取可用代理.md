---
title: 爬虫系列(五)---获取可用代理
categories: 
  - 爬虫
tags:
  - 爬虫
  - python3
  
date: 2018-12-10
mathjax: true

---

```python

import time
import re
import requests 
from bs4 import BeautifulSoup
from requests.exceptions import RequestException

def parse_ip(http,ip,port):
    try:
        proxies = {"http": "http://"+ip+":"+port}
        print(proxies)
        resp2 = requests.get("http://2018.ip138.com/ic.asp", proxies=proxies)
        soup = BeautifulSoup(resp2.content.decode("gbk"), "html.parser")
        return soup.body.get_text()
    except RequestException as e:
        return None 

def get_proxy(index):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36"
    }
    html = requests.get(f"https://www.xicidaili.com/nn/{index}",headers=headers)
    #print(html.text)
    soup = BeautifulSoup(html.content,"lxml")
    get_info = soup.find_all("tr")
    #print(get_info[1])
    for i in get_info:
        get_ip = re.match("<tr.+?>.<td.+?></td>.<td>(.+?)</td>.<td>(.+?)</td>.+(HTTP\w)",str(i),re.S)
        # <tr.+?>.<td.+?></td>.<td>(.+?)</td>.<td>(.+?)</td>.+(HTTP\w)
        if get_ip:
            #print(f"{get_ip.group(3)}://{get_ip.group(1)}:{get_ip.group(2)}")
            ip = parse_ip(get_ip.group(3),get_ip.group(1),get_ip.group(2))
            #print(ip)
            time.sleep(10)
            if ip:
                return ip
            else:
                continue

def main():
    for index in range(10):
        print(get_proxy(index))

if __name__ == "__main__":
    main()

```